# 第十二章 集成学习
Ensemble Learning <br>
面对一个机器学习问题，通常有两种策略。
- 一种是研发人员尝试各种模型，选择其中表现最好的模型做重点调参优化。
这种策略类似于奥运会比赛，通过强强竞争来选拔最优的运动员，并逐步提高成绩。
- 另一种重要的策略是集各家之长，如同贤明的君主广泛地听取众多谋臣的建议，然后综合考虑，得到最终决策。
后一种策略的核心，是将多个分类器的结果统一成一个最终的决策。使用这类策略的机器学习方法统称为集成学习。其中的每个单独的分类器称为基分类器。

## 01 集成学习的种类
集成学习是一大类模型融合策略和方法的统称，其中包含多种集成学习的思想。

### 集成学习分哪几种？他们有何异同？
- Boosting
    - Boosting方法训练基分类器时采用**串行**的方式，各个基分类器之间有依赖。
    - 它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。
    测试时，根据各层分类器的结果的加权得到最终结果。
    - Boosting的过程很类似于人类学习的过程，我们学习新知识的过程往往是迭代式的，第一遍学习的时候，
    我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。
    第二遍学习的时候，就会针对犯过错误的知识加强学习，以减少类似的错误发生。不断循环往复，直到犯错误的次数减少到很低的程度。

- Bagging
    - Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行**并行**训练。
    - 其中很著名的算法之一是基于决策树基分类器的随机森林（Random Forest）。
    为了让基分类器之间互相独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。
    - Bagging方法更像是一个集体决策的过程，每个个体都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠。
    但由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独作出判断，
    再通过投票的方式做出最后的集体决策。
    
- 基分类器  
    - 有时又被称为弱分类器，因为基分类器的错误率要大于集成分类器。
    - 基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，
    表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。
    
- Boosting 和 Bagging 的区别
    - Boosting方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。
    - Bagging方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，
    来减小集成分类器的方差。
    - 假设所有基分类器出错的概率是独立的，在某个测试样本上，用简单多数投票方法来集成结果，
    超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

如图所示Bagging算法，Model 1、Model 2、Model 3都是用训练集的一个子集训练出来的，单独来看，它们的决策边界都很曲折，
有过拟合的倾向。集成之后的模型（红线所示）的决策边界就比各个独立的模型平滑了，这是由于集成的加权投票方法，减小了方差。
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_01.png?raw=true)

## 02 集成学习的步骤和例子
虽然集成学习的具体算法和策略各不相同，但都共享同样的基本步骤。

### 集成学习有哪些基本步骤？请举几个集成学习的例子。
- 集成学习一般可以分为以下3个步骤：
    1. 找到误差互相独立的基分类器
    2. 训练基分类器
    3. 合并基分类器的结果

- 合并基分类器的方法：
    1. Voting: 用投票的方式，将获得最多选票的结果作为最终的结果。
    2. Stacking: 用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加
    （或者用更复杂的算法融合，比如把各基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。

- 集成学习的例子：
    1. Adaboost-自适应Boosting（Adaptive Boosting）<br>
    其基分类器的训练和合并的基本步骤如下：
        1. 确定基分类器：这里可以选取ID3决策树作为基分类器。
        事实上，任何分类模型都可以作为基分类器，但树形模型由于结构简单且较易产生随机性所以比较常用。
        2. 训练基分类器：假设训练集为$\lbrace x_i, y_i \rbrace, i=1,...,M$，其中$y_i \in \lbrace -1, 1 \rbrace$，
        并且有$T$个基分类器，则可以按照如下过程来训练基分类器：
            - 初始化采样分布 $ D_1(i) = \frac {1}{M} $；
            - 令$ t = 1,2,...,T $循环：
                - 从训练集中，按照$D_t$分布，采样出子集$S_t = \lbrace x_i, y_i \rbrace, i=1,...,M_t $；
                - 用$S_t$训练出基分类器 $h_t$；
                - 计算$h_t$的错误率：
                $ \epsilon_t = \frac{\sum_{i=1}^{M_t} I[h_t(x_i) \neq y_i]D_t(x_i)}{M_t}$
                其中$I[\quad]$为判别函数；
                - 计算基分类器$h_t$权重 $\alpha_t = \log \frac{1 - \epsilon_t}{\epsilon_t}$；
                - 设置下一次采样
                $$ D_{t+1} = \begin{cases} D_t(i)或者\frac{D_t(i)(1 - \epsilon_t)}{\epsilon_t}, h_t(x_i) \neq y_i; \quad \frac{D_t(i) \epsilon_t}{(1 - \epsilon_t)}, & h_t(x_i) = y_i \end{cases} $$
                并将它归一化为一个概率分布函数。
        3. 合并基分类器：给定一个位置样本$z$，输出分类结果为加权投票的结果$Sign(\sum_{t=1}^T h_t(z) \alpha_t)$。
        
        从Adaboost的例子中可以明显地看到Boosting的思想，对分类正确的样本降低了权重，对分类错误的样本升高或者保持权重不变。
        在最后进行模型融合的过程中，也根据错误率对基分类器进行加权融合。错误率低的分类器拥有更大的“话语权”。
        
    2. GBDT-梯度提升决策树（Gradient Boosting Decision Tree）
        - 核心思想：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。
        - 这里使用残差继续学习，就是GBDT中Gradient Boosting所表达的意思。
        - GBDT原理详见下文。

## 03 基分类器
基分类器的选择是集成学习主要步骤中的第一步，也是非常重要的一步。
到底选择什么样的基分类器，为什么很多集成学习模型都选择决策树作为基分类器，这些都是需要明确的问题，
做到知其然，也知其所以然。

### 常用的基分类器是什么？
最常用的基分类器是决策树，主要有以下3个方面的原因：
1. 决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。
2. 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。
3. 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，
这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，
很好地引入了随机性。

除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型也比较“不稳定”，
而且还可以通过调整神经元数量、连接方式、网络层数、初始权值等方式引入随机性。

### 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-NN？请解释为什么？
随机森林属于Bagging类的集成学习。**Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。**
Bagging所采用的基分类器，最好是本身对样本分布较为敏感的（即所谓不稳定的分类器），这样Bagging才能有用武之地。
线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大，
所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，
而导致他们在训练中更难收敛，从而增大了集成分类器的偏差。

## 04 偏差与方差
经常用过拟合、欠拟合来定性地描述模型是否很好地解决了特定的问题。从定量的角度来说，
可以用模型的偏差（Bias）与方差（Variance）来描述模型的性能。集成学习往往能够“神奇”地提升弱分类器的性能。

### 什么是偏差和方差
在有监督学习中，模型的泛化误差来源于两个方面——偏差和方差。
- 偏差 <br>
偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差。<br>
偏差通常是由于对学习算法做了错误的假设所导致的，比如真实模型是某个二次函数，但假设模型是一次函数。
由偏差带来的误差通常在训练误差上就能体现出来。

- 方差 <br>
方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。<br>
方差通常是由于模型的复杂度相对于训练样本数m过高导致的，比如一共有100个训练样本，
而假设模型是阶数不大于200的多项式函数。由方差带来的误差通常体现在测试误差相对于训练误差的增量上。

偏差和方差示意图 <br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_02.png?raw=true)

### 如何从减小方差和偏差的角度解释Boosting和Bagging的原理？
简单回答这个问题就是：**Bagging能够提高弱分类器性能的原因是降低了方差，Boosting能够提升弱分类器性能的原因是降低了偏差。**

- Bagging是Bootstrap Aggregating的简称，意思是再抽样，然后在每个样本上训练出来的模型取平均。
    - 假设有$n$个随机变量，方差即为$\sigma^2$，在随机变量完全独立的情况下，$n$个随机变量的方差为$\frac{\sigma^2}{n}$，
    也就是说方差减小到了原来的$\frac{1}{n}$。
    - 再从模型的角度看，对$n$个独立不相关的模型的预测结果取平均，方差是原来单个模型的$\frac{1}{n}$。
    这个描述不甚严谨，但原理已经讲得很清楚了。当然，模型之间不可能完全独立。为了追求模型的独立性，
    诸多Bagging的方法做了不同的改进。比如在随机森林算法中，每次选取节点分裂属性时，会随机抽取一个属性子集，
    而不是从所有属性中选取最优属性，这就是为了避免弱分类器之间过强的相关性。
    通过训练集的重采样也能够带来弱分类器之间的一定独立性，从而降低Bagging后模型的方差。
    
- Boosting <br>
Boosting的训练过程：在训练好一个弱分类器后，需要计算弱分类器的错误或者残差，作为下一个分类器的输入。
这个过程本身就是在不断减小损失函数，来使模型不断逼近“靶心”，使得模型偏差不断降低。
但Boosting的过程并不会显著降低方差。这是因为Boosting的训练过程使得各弱分类器之间是强相关的，
缺乏独立性，所以并不会对降低方差有作用。

方差和偏差是相辅相成，矛盾又统一的，二者并不能完全独立的存在。对于给定的学习任务和训练数据集，
需要对模型的复杂度做合理的假设。如果模型复杂度过低，虽然方差很小，但是偏差会很高；
如果模型复杂度过高，虽然偏差降低了，但是方差会很高。所以需要综合考虑偏差和方差选择合适复杂度的模型进行训练。

如图所示，泛化误差、偏差、方差和模型复杂度的关系。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_03.png?raw=true)

## 05 梯度提升决策树的基本原理
梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是Boosting算法中非常流行的模型，
也是近来在机器学习竞赛、商业应用中表现都非常优秀的模型。GBDT非常好地体现了“从错误中学习”的理念，
基于决策树预测的残差进行迭代的学习。

### GBDT的基本原理是什么？
Boosting和Bagging是两大集成学习的框架。
相比于Bagging中各个弱分类器可以独立地进行训练，Boosting中的弱分类器需要依次生成。
在每一轮迭代中，基于已生成的弱分类器集合（即当前模型）的预测结果，新的弱分类器会重点关注那些还没有被正确预测的样本。

Gradient Boosting是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，
然后将训练好的弱分类器以累加的形式结合到现有模型中。

Gradient Boosting算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，
然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

采用决策树作为弱分类器的Gradient Boosting算法被称为GBDT，有时又被称为MART（Multiple Additive Regression Tree）。
GBDT中使用的决策树通常为CART。

由于GBDT是利用残差训练的，在预测的过程中，也需要把所有树的预测值加起来，得到最终的预测结果。

GBDT算法的伪代码如下: <br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_04.png?raw=true)

GBDT训练过程举例如图：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_06.png?raw=true)
> [参考：图例过程解释](https://blog.csdn.net/puqutogether/article/details/41957089)

#### 原理详解：
> [参考：GBDT基本原理及算法描述](https://blog.csdn.net/qq_24519677/article/details/82020863)
> [参考：GBDT算法原理以及实例理解](https://blog.csdn.net/zpalyq110/article/details/79527653) 

GBDT算法是模型为加法模型，学习算法为前向分步算法，基函数为CART树，损失函数为平方损失函数的回归问题，
为指数函数的分类问题和为一般损失函数的一般决策问题。

在针对基学习器的不足上，梯度提升算法是通过算梯度来定位模型的不足。当GBDT的损失函数是平方损失时，即
$ L(y,f(x)) =  \frac{1}{2} (y - f(x))^2$ 时，则负梯度 $ - \frac{\partial L}{\partial f(x)} = y - f(x) $，
而$ y-f(x) $即为我们所说的残差，GBDT的思想就是在每次迭代中拟合残差来学习一个弱学习器。而残差的方向即为我们全局最优的方向。

但是当损失函数不为平方损失时，该如何拟合弱学习器呢？Friedman提出使用损失函数负梯度的方向代替残差方向，
我们称损失函数负梯度为伪残差。而伪残差的方向即为我们局部最优的方向。所以在GBDT中，当损失函数不为平方损失时，
用每次迭代的局部最优方向代替全局最优方向。

可以证明，当损失函数为平方损失时，叶节点中使平方损失误差达到最小值的是叶节点中所有值的均值；
而当损失函数为绝对值损失时，叶节点中使绝对损失误差达到最小值的是叶节点中所有值的中位数。

1. Decision Tree: CART回归树 <br>
首先，GBDT使用的决策树是CART回归树，无论是处理回归问题还是二分类以及多分类，GBDT使用的决策树通通都是都是CART回归树。
为什么不用CART分类树呢？因为GBDT每次迭代要拟合的是**梯度值**，是**连续值**所以要用回归树。<br><br>
对于回归树算法来说最重要的是寻找最佳的划分点，那么回归树中的可划分点包含了所有特征的所有可取的值。
在分类树中最佳划分点的判别标准是熵或者基尼系数，都是用纯度来衡量的，但是在回归树中的样本标签是连续数值，
所以再使用熵之类的指标不再合适，取而代之的是平方误差，它能很好的评判拟合程度。<br><br>
**回归树生成算法**：<br>
输入：训练数据集$D$。<br>
输出：回归树$f(x)$。<br>
在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。
假设节点$R$中有$N$个样本点，$j$为切分变量(或切分特征)，$s$为切分点，$R_1，R_2$分别为切分后的左节点和右节点，
分别有节点个数为$N_1, N_2$。我们的目标是找到切分变量$j$和切分点$s$，在$R_1，R_2$内部使平方损失误差达到最小值的$c_1, c_2$，如下：
    1. 选择最优切分变量$j$与切分点$s$，求解：
    $$\min_{j,s} \bigg[ \min_{c1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \bigg]$$
    遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使得上式达到最小值的对$(j,s)$。
    2. 用选的的对$(j,s)$划分区域并决定相应的输出值：
    $$ R_1(j,s) = \lbrace x|x^{(j)} \leq s \rbrace \quad and \quad R_2(j,s) = \lbrace x|x^{(j)} > s \rbrace $$<br>
    要求最小值，则对$\sum_{x_i \in R_1(j,s} (y_i - c_1)^2$和$\sum_{x_i \in R_2(j,s} (y_i - c_2)^2$分别对$c_1$和$c_2$求偏导，
    并令偏导数为0，得到在$ R_1，R_2 $内部使平方损失误差达到最小值的$ c_1, c_2$：
    $$ c_1 = \frac{1}{N_1} \sum_{x_i \in R_1(j,s} y_i ; \quad c_2 = \frac{1}{N_2} \sum_{x_i \in R_2(j,s} y_i $$<br>
    而$c_1,c_2$即为各自叶节点中的残差的均值，合并后写成如下：
    $$ \hat{c_m} = \frac{1}{N} \sum_{x_i \in R_m(j,s)} y_i, \quad x_i \in R_m, m = 1,2 $$
    3. 继续对两个子区域调用步骤(i)和(ii)，直到满足停止条件。
    4. 将输入空间划分为$M$个区域$ R_1, R_2,...,R_M$，生成决策树：
    $$ f(x) = \sum_{m=1}^M \hat{c_m}I(x \in R_m) $$
    
2. Gradient Boosting: 拟合负梯度 <br>
梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法。<br>
先来个通俗理解：假如有个人30岁，首先用20岁去拟合，发现损失有10岁，这时用6岁去拟合剩下的损失，发现差距还有4岁，
第三轮用3岁拟合剩下的差距，差距就只有一岁了。如果迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。
最后将每次拟合的岁数加起来便是模型输出的结果。

    **提升树算法**:
    1. 初始化$ f_0(x) = 0 $
    2. 对$ m = 1,2,...,M $:
        1. 计算残差
        $$ r_{mi} = y_i - f_{m-1}(x), \quad i=1,2,...,N $$
        $m$表示样本集，$i$表示样本集中的样本。
        2. 拟合残差$r_{mi}$学习一个回归树，得到$h_m(x)$。
        3. 更新$f_m(x) = f_{m-1} + h_m(x)$。
    3. 得到回归问题提升树
    $$ f_M(x) = \sum_{m=1}^M h_m(x) $$
    
    **残差**:
    1. 在提升树算法中，假设前一轮迭代得到的强学习器是$ f_{t-1}(x) $
    2. 损失函数是$ L(y, f_{t-1}(x)) $
    3. 本轮迭代的目标是找到一个弱学习器$ h_t(x) $
    4. 最小化本轮的损失$ L(y,f_t(x)) = L(y, f_{t-1}(x) + h_t(x)) $
    5. 当采用评分损失函数时
    $$ L(y, f_{t-1}(x) + h_t(x)) = \bigg(y - (f_{t-1}(x) + h_t(x))\bigg)^2 = (r - h_t(x))^2 $$
    6. 这里$ r = y - f_{t-1}(x) $是当前模型拟合数据的残差(residual)，
    所以，对于提升树来说，只需要简单地拟合当前模型的残差。
    
    当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，
    但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，
    Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，
    **其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值**。
    
    **负梯度**:
    1. 第$t$轮的第$i$个样本的损失函数的负梯度为：
    $$ - \bigg[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \bigg] \_{f(x) = f_{t-1}(x)} $$
    2. 此时不同的损失函数将会得到不同的负梯度，如果选择平方损失：
    $$ L(y,f(x_i)) = \frac{1}{2} (y - f(x_i))^2 $$
    3. 负梯度为：
    $$ - \bigg[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \bigg] \_{f(x) = f_{t-1}(x)} = y - f(x_i) $$
    4. 此时我们发现GBDT的**负梯度就是残差**，所以说对于回归问题，要拟合的就是残差。
    5. 对于分类问题，二分类和多分类的损失函数都是$\log loss$

3. GBDT算法基本模板 <br>
上面两部分分别将Decision Tree和Gradient Boosting介绍完了，下面将这两部分组合在一起就是GBDT了，这里以回归问题进行推导。<br>
输入：训练数据集$T= \lbrace (x_1,y_1), (x_2,y_2),...,(x_N,y_N)\rbrace $,损失函数为$L(y,f(x))$。<br>
输出：回归树$F(x)$
    1. 初始化：
        1. 初始化弱学习器
        (估计使损失函数极小化的常数值，它是只有一个根节点的树，一般平方损失函数为节点的均值，而绝对损失函数为节点样本的中位数）：
        $$ f_0(x) = arg \min_c \sum_{i=1}^N L(y_i, c) $$
        2. 损失函数为平方损失，因为平方损失函数是一个凸函数，直接求导：
        $$ \sum_{i=1}^N \frac{\partial L(y_i,c)}{\partial c} = \sum_{i=1}^N \frac{\partial (\frac{1}{2}) (y_i - c)^2}{\partial c} = \sum_{i=1}^N c - y_i $$
        3. 令导数等于0：
        $$ \sum_{i=1}^N c - y_i = 0 $$
        $$ c = \frac{\sum_{i=1}^N y_i}{N} $$
        4. 所以初始化时，$c$取值为所有训练样本标签值的均值，此时得到初始学习器$f_0(x)$为：
        $$ f_0(x) = c $$
    
    2. 对迭代轮数$ m = 1,2,...,M $(M表示迭代次数，即生成的弱学习器个数)有：
        1. 对给个样本$ i=1,2,...,N $，计算负梯度，即残差：
        $$ r_{mi} = - \bigg[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \bigg] \_{f(x) = f_{m-1}(x)} $$
        2. 将上一步得到的残差作为样本新的真实值，并将数据$ (x_i,r_{mi}), i=1,2,...,N $作为下棵树的训练数据，
        得到一颗新的回归树$ f_m(x) $，其对应的叶子节点区域为$ R_{mj}, j=1,2,...,J $，
        其中$J$为回归树$t$的叶子节点的个数。
        3. 对叶子区域$ j=1,2,...,J $计算最佳拟合值：
        $$ \Upsilon_{m,j} = \underbrace{arg \min}\_{\Upsilon} \sum_{x_i \in R_{mj}} L(y_i, f_{m-1}(x_i) + \Upsilon ) $$
        4. 更新强学习器：
        $$ f_m(x) = f_{m-1}(x) + \sum_{j=1}^J \Upsilon_{mj}I, \quad (x \in R_{mj}) $$
    
    3. 得到最终学习器：
    $$ f(x) = f_M(x) = f_0(x) + \sum_{m=1}^M \sum_{j=1}^J \Upsilon_{mj}I, \quad (x \in R_{mj}) $$
    
    4. 运用Shrinkage的思想，增加学习率$\alpha$：<br>
    增加学习率的原因是，如果每次都全部加上（学习率为1）很容易一步学到位导致过拟合。
    $$ f(x) = f_M(x) = f_0(x) + \alpha \sum_{m=1}^M \sum_{j=1}^J \Upsilon_{mj}I, \quad (x \in R_{mj}) $$
    
### 梯度提升和梯度下降的区别和联系是什么？
两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，
模型是以参数化形式表示，从而模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，
而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_05.png?raw=true)

### GBDT的优点和局限性有哪些？
- 优点：
    - 预测阶段的计算速度快，树与树之间可并行化计算。
    - 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
    - 采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，
    并且也不需要对数据进行特殊的预处理如归一化等。

- 局限性：
    - GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
    - GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
    - 训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。
    
## 06 XGBoost
XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，
被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。
在使用XGBoost平台的时候，也需要熟悉XGBoost平台的内部实现和原理，
这样才能够更好地进行模型调参并针对特定业务场景进行模型改进。

### XGBoost的原理是什么？
> [参考：论文原文](https://github.com/pchen12567/Machine_Learning_Interview/blob/master/Ch12_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/XGBoost/XGBoost.pdf)
>
> [参考](https://zhuanlan.zhihu.com/p/40129825)

XGBoost是boosting算法的其中一种。
Boosting算法的思想是将许多弱分类器集成在一起形成一个强分类器。

因为XGBoost是一种提升树模型，所以它是将许多树模型集成在一起，形成一个很强的分类器。

XGBoost Tree由多个CART集成，按照策略选取最佳分隔点，对稀疏数据进行处理，并加入了例如并行化等优化方法。

1. CART回归树 <br>
    CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，
    设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。
    $$ R_1(j,s) = \lbrace x|x^{(j)} \leq s \rbrace \quad and \quad R_2(j,s) = \lbrace x|x^{(j)} > s \rbrace $$

    而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，
    因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：
    $$ \sum_{x_i \in R_m} (y_i - f(x_i))^2 $$
    
    因此，当为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：
    $$ \min_{j,s} \bigg[ \min_{c1} \sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)} (y_i - c_2)^2 \bigg] $$
    
    所以只要遍历所有特征的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。

2. XGBoost算法思想 <br>
    该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。
    当训练完成得到$k$棵树，要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，
    每个叶子节点就对应一个分数，最后只需要将每棵树对应的分数加起来就是该样本的预测值。
    $$ \hat{y} = \phi (x_i) = \sum_{k=1}^K f_k(x_i) $$
    $$ where \quad F = \lbrace f(x) = w_{q(x)} \rbrace \quad (q:R^m \rightarrow T, w \in R^T) $$
    一个含有$n$个样本$m$个特征的数据集，其中$q$代表每颗树中叶子节点的索引；$T$代表一颗树上的叶子数量；
    $f_k$代表一颗叶子节点为$q$叶子的权重为$w$独立的回归树；由于回归树的每个叶子上的权重是一个连续值，
    因此用$w_i$代表$i-th$叶子上的权重。<br>
    如下图例子，训练出了2棵决策树，小孩的预测分数就是两棵树中小孩所落到的结点的分数相加。爷爷的预测分数同理。<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/XGBoost_01.png?raw=true)

3. XGBoost原理
    1. XGBoost目标函数定义为：
    $$ Obj^{(t)} = L(\phi)^{(t)} = \sum_{i=1}^n l(y_i, \hat{y_i}^{(t)}) + \sum_{k=1}^K \Omega (f_k) $$
    $$ where \quad \Omega(f) = \gamma T + \frac{1}{2} \lambda ||w||^2 $$
    目标函数由两部分构成，第一部分是经验误差，用来衡量预测分数和真实分数的差距，另一部分则是正则化项。
    其中是$y_i$真实值，$\hat{y_i}^{(t)}$是第$t$轮的预测值，$l()$是经验损失函数，$\Omega$是正则项，防止过拟合或欠拟合，
    $f$是一颗CART树，$f_k(x_i)$即输入$x_i$的第$k$课树的输出值。正则化项同样包含两部分，$T$表示叶子结点的个数，
    $w$表示叶子节点的分数。$\gamma$可以控制叶子结点的个数，$\lambda$可以控制叶子节点的分数不会过大，防止过拟合。
    
    2. 第$t$轮的预测值：<br>
    新生成的树是要拟合上次预测的残差的，即当生成t棵树后，预测分数可以写成：
    $$ \hat{y_i}^{(0)} = 0 $$
    $$ \hat{y_i}^{(1)} = f_1(x_i) = \hat{y_i}^{(0)} + f_1(x_i) $$
    $$ \hat{y_i}^{(2)} = f_1(x_i) + f_2(x_i) = \hat{y_i}^{(1)} + f_2(x_i) $$
    $$ ... $$
    $$ \hat{y_i}^{(t)} = \sum_{k=1}^t f_k(x_i) = \hat{y_i}^{(t-1)} +f_t(x_i) $$
    
    3. 目标函数改写为：
    $$ Obj^{(t)} = L(\phi)^{(t)} = \sum_{i=1}^n l(y_i, \hat{y_i}^{(t-1)} + f_t(x_i)) + \Omega(f_t) $$
    此时，就是要找到一个$f_t$能够最小化目标函数。XGBoost的想法是利用其在$f_t = 0$处的泰勒二阶展开近似它。
    
    4. 泰勒二阶展开式：
    $$ f(x + \Delta x) \approx f(x) + f'(x) \Delta x + \frac{1}{2}f''(x) \Delta x^2 $$
    对应二阶泰勒展开式，这里$x$即为$\hat{y_i}^{(t-1)}$，而$\Delta x$即为$f_t(x_i)$，可以将目标函数近似为：
    $$ L(\phi)^{(t)} \approx \sum_{i=1}^n \bigg[ l(y_i, \hat{y_i}^{(t-1)}) + g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i) \bigg] + \Omega(f_t) $$
    其中$g_i$为一阶导数，$h_i$为二阶导数：
    $$ g_i = \partial_{\hat{y}^{(t-1)}} l(y_i, \hat{y}^{(t-1)}), \quad h_i = \partial_{\hat{y}^{(t-1)}}^2 l(y_i, \hat{y}^{(t-1)}) $$
    
    5. 去掉常数项：<br>
    由于前$t-1$颗树的预测值与$y$的残差对目标函数优化不影响，可以直接去掉$l(y_i, \hat{y_i}^{(t-1)})$。简化目标函数为：
    $$ \tilde{L}^{(t)} = \sum_{i=1}^n \bigg[ g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i) \bigg] + \Omega(f_t) $$
    
    6. 重组叶子节点样本：<br>
    上式是将每个样本的损失函数值加起来，由于每个样本都最终会落到一个叶子结点中，所以可以将所有同一个叶子结点的样本重组起来，
    然后将正则项展开，合并得到：
    $$ \tilde{L}^{(t)} = \sum_{i=1}^n \bigg[ g_if_t(x_i) + \frac{1}{2}h_if_t^2(x_i) \bigg] + \Omega(f_t) $$
    $$ => \tilde{L}^{(t)} = \sum_{i=1}^n \bigg[ g_i f_t(x_i) + \frac{1}{2} h_i f_t(x_i)^2 \bigg] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 $$
    $$ => \tilde{L}^{(t)} = \sum_{i=1}^n \bigg[ g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2 \bigg] + \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_i^2 $$
    $$ => \tilde{L}^{(t)} = \sum_{j=1}^T \bigg[ (\sum_{i \in I_j}g_i) w_j + \frac{1}{2} (\sum_{i \in I_j} h_i + \lambda) w_j^2 \bigg] + \gamma T $$
    $$ => \tilde{L}^{(t)} = \sum_{j=1}^T \bigg[ G_j w_j +\frac{1}{2} (H_j +\lambda) w_j^2 \bigg] + \gamma T $$
    
    7. 一元二次方程顶点式：<br>
    对于一元二次方程：$ y = ax^2 + bx + c $，其顶点式为：$ y = a(x - h)^2 + k $，其顶点坐标$(h,k)$为：
    $$ (-\frac{b}{2a}, \frac{4ac - b^2}{4a}) $$
    
    8. 求目标函数的最小值：<br>
    由于目标函数在顶点位置时为最小值，根据一元二次方程的顶点式，此时最优的$w$为：
    $$ w_j^* = -\frac{G_j}{H_j + \lambda} $$
    将最优$w$带入到目标函数中，得到此时的目标函数为：
    $$ \tilde{L}^{(t)} = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j + \lambda} + \gamma T $$
    
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/XGBoost_02.png?raw=true)

4. 分割节点算法<br>
    在上面的推导中，如果一棵树的结构确定了，可以求得每个叶子结点的分数。但还没介绍如何确定树结构，
    即每次特征分裂怎么寻找最佳特征，怎么寻找最佳分裂点。
    
    正如上文说到，基于空间切分去构造一颗决策树是一个NP难问题，不可能去遍历所有树结构，因此，
    XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用上式目标函数值作为评价函数。
    具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。
    
    同时可以设置树的最大深度、当样本权重之和小于设定阈值时停止生长去防止过拟合。
    
    假设$I_L$和$I_R$分别为样本分割后的左、右样本子集，满足$I = I_L \cup I_R$，分割后的损失函数为：
    $$ Gain = L_{split} = \frac{1}{2} \bigg[ \frac{(\sum_{i \in I_L } g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R } g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I } g_i)^2}{\sum_{i \in I} h_i + \lambda} \bigg] - \gamma $$
    
    **4.1 基础贪心算法 (Basic Exact Greedy Algorithm)** <br>
    在所有特征上遍历，每个特征中选择该特征下的每个值作为其分裂点，计算增益损失。当遍历完所有特征之后，
    增益损失最大的特征值将作为其分裂点。由此可以看出这其实就是一种穷举算法，而整个树构造过程最耗时的过程就是寻找最优分裂点的过程。
    
    缺点是：**当数据量过大，贪心算法就不好用了，因为要遍历每个分割点，甚至内存都放不下，不能高效操。**
    
    具体算法伪代码如下：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/XGBoost_03.png?raw=true)
    
    **4.2 近似算法 (Approximate Algorithm)** <br>
    对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。
    
    XGBoost的思想，是根据特征分布的百分数采样，选择待分隔点，将连续的特征映射到一个桶（buckets）中，即找到$l$个划分点，
    将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。
    基于统计信息选择最佳分隔方案。选择分隔点的方式有两个，一种是global，一种是local。
    
    **global**是在树构建的初始阶段选出所有候选分隔点，后面每层都使用相同的策略选择分隔点；
    全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分。
    
    **local**在每次split后重新选出候选分隔点，适合较深的树，这样步骤比global多。
    局部近似就是在具体的某一次分裂节点的过程中采用近似算法。
    
    综上，local只需要较少的候选点，而global必须要有足够多的候选点（桶）才能达到和local差不多的精确度。
    对每一个特征进行「值」采样，原来需要对每一个特征的每一个可能分割点进行尝试，采样之后只针对采样的点进行分割尝试，
    这种方法很明显可以减少计算量，采样密度越小计算的越快，拟合程度也会越差，所以采样还可以防止过拟合。
    
    具体算法伪代码如下：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/XGBoost_04.png?raw=true)
    
    **4.3 稀疏感知的分割点查找算法 (Sparsity-aware Split Finding)** <br>
    该算法主要是针对存在缺失值的情况，当样本的第$i$个特征值缺失时，无法利用该特征进行划分时，
    XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。
    
    具体算法伪代码如下：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/XGBoost_05.png?raw=true)
    
    **4.4 带权重直方图算法 (Weighted Quantile Sketch)**
    - 样本的所有第$k$个特征构造一个集合：{特征的值，二阶导数}
    - 序函数表示第$k$个特征的值小于$z$的样本比例，为了找到满足相邻两个分隔点的值相差不大于阈值$e$的分隔点。
    - 相当于将特征的值排序，在这个序列上根据计算带权的序函数选择合适的分隔点，使得每个含有不同权重的特征值区间分布均匀。
    - 对于带权重的数据集合，此前还没有直方图算法来处理。
    
    构造略图（sketching）是指使用随机映射（Random projections）将数据流投射在一个小的存储空间内作为整个数据流的概要，
    这个小空间存储的概要数据称为略图，可用于近似回答特定的查询。不同的略图可用于对数据流的不同Lp范数的估算，
    进而这些Lp范数可用于回答其它类型的查询。如L0范数可用于估算数据流的不同值(distinct count)；
    L1范数可用于计算分位数（quantile）和频繁项（frequent items）；L2范数可用于估算自连接的长度等等。
    
5. 防止过拟合<br>
    XGBoost还提出了两种防止过拟合的方法：Shrinkage and Column Subsampling。
    
    - 缩减(Shrinkage)<br>
    Shrinkage方法就是在每次迭代中对树的每个叶子结点的分数乘上一个缩减权重$\alpha$，这可以使得每一棵树的影响力不会太大，
    留下更大的空间给后面生成的树去优化模型。类似于随机优化过程中的学习率，减少单棵树的影响。
    
    - 对列的二次采样(Column Subsampling)<br>
    Column Subsampling类似于随机森林中的选取部分特征进行建树。其可分为两种，一种是按层随机采样，在对同一层内每个结点分裂之前，
    先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。另一种是随机选择特征，
    则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。

6. XGBoost的优点
    - 使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。
    - 目标函数优化利用了损失函数关于待求函数的二阶导数。
    - 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。
    具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。
    - 添加了对稀疏数据的处理。
    - 交叉验证，early stop，当预测结果已经很好的时候可以提前停止建树，加快训练速度。
    - 支持设置样本权重，该权重体现在一阶导数g和二阶导数h，通过调整权重可以去更加关注一些样本。

7. 系统设计(To be continue...)

### XGBoost与GBDT的联系和区别有哪些？
原始的GBDT算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝。
而XGBoost在决策树构建阶段就加入了正则项，即：
$$ Obj^{(t)} = L(\phi)^{(t)} = \sum_{i=1}^n l(y_i, \hat{y_i}^{(t-1)} + f_t(x_i)) + \Omega(f_t) $$
根据计算出分裂前后损失函数的差值为：
$$ Gain = L_{split} = \frac{1}{2} \bigg[ \frac{(\sum_{i \in I_L } g_i)^2}{\sum_{i \in I_L} h_i + \lambda} + \frac{(\sum_{i \in I_R } g_i)^2}{\sum_{i \in I_R} h_i + \lambda} - \frac{(\sum_{i \in I } g_i)^2}{\sum_{i \in I} h_i + \lambda} \bigg] - \gamma $$
XGBoost采用最大化这个差值作为准则来进行决策树的构建，通过遍历所有特征的所有取值，寻找使得损失函数前后相差最大时对应的分裂方式。
此外，由于损失函数前后存在差值一定为正的限制，此时$\gamma$起到了一定的预剪枝效果。

1. GBDT是机器学习算法，XGBoost是该算法的工程实现。
2. 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
3. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
4. 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。
5. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
6. 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺失值的处理策略。