# 第十二章 集成学习
Ensemble Learning <br>
面对一个机器学习问题，通常有两种策略。
- 一种是研发人员尝试各种模型，选择其中表现最好的模型做重点调参优化。
这种策略类似于奥运会比赛，通过强强竞争来选拔最优的运动员，并逐步提高成绩。
- 另一种重要的策略是集各家之长，如同贤明的君主广泛地听取众多谋臣的建议，然后综合考虑，得到最终决策。
后一种策略的核心，是将多个分类器的结果统一成一个最终的决策。使用这类策略的机器学习方法统称为集成学习。其中的每个单独的分类
器称为基分类器。

## 01 集成学习的种类
集成学习是一大类模型融合策略和方法的统称，其中包含多种集成学习的思想。

### 集成学习分哪几种？他们有何异同？
- Boosting
    - Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。
    - 它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。
    测试时，根据各层分类器的结果的加权得到最终结果。
    - Boosting的过程很类似于人类学习的过程，我们学习新知识的过程往往是迭代式的，第一遍学习的时候，
    我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。
    第二遍学习的时候，就会针对犯过错误的知识加强学习，以减少类似的错误发生。不断循环往复，直到犯错误的次数减少到很低的程度。

- Bagging
    - Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。
    - 其中很著名的算法之一是基于决策树基分类器的随机森林（Random Forest）。
    为了让基分类器之间互相独立，将训练集分为若干子集（当训练样本数量较少时，子集之间可能有交叠）。
    - Bagging方法更像是一个集体决策的过程，每个个体都进行单独学习，学习的内容可以相同，也可以不同，也可以部分重叠。
    但由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独作出判断，
    再通过投票的方式做出最后的集体决策。
    
- 基分类器  
    - 有时又被称为弱分类器，因为基分类器的错误率要大于集成分类器。
    - 基分类器的错误，是偏差和方差两种错误之和。偏差主要是由于分类器的表达能力有限导致的系统性错误，
    表现在训练误差不收敛。方差是由于分类器对于样本分布过于敏感，导致在训练样本数较少时，产生过拟合。
    
- Boosting 和 Bagging 的区别
    - Boosting方法是通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。
    - Bagging方法则是采取分而治之的策略，通过对训练样本多次采样，并分别训练出多个不同模型，然后做综合，
    来减小集成分类器的方差。
    - 假设所有基分类器出错的概率是独立的，在某个测试样本上，用简单多数投票方法来集成结果，
    超过半数基分类器出错的概率会随着基分类器的数量增加而下降。

如图所示Bagging算法，Model 1、Model 2、Model 3都是用训练集的一个子集训练出来的，单独来看，它们的决策边界都很曲折，
有过拟合的倾向。集成之后的模型（红线所示）的决策边界就比各个独立的模型平滑了，这是由于集成的加权投票方法，减小了方差。
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_01.png?raw=true)

## 02 集成学习的步骤和例子
虽然集成学习的具体算法和策略各不相同，但都共享同样的基本步骤。

### 集成学习有哪些基本步骤？请举几个集成学习的例子。
- 集成学习一般可以分为以下3个步骤：
    1. 找到误差互相独立的基分类器
    2. 训练基分类器
    3. 合并基分类器的结果

- 合并基分类器的方法：
    1. Voting: 用投票的方式，将获得最多选票的结果作为最终的结果。
    2. Stacking: 用串行的方式，把前一个基分类器的结果输出到下一个分类器，将所有基分类器的输出结果相加
    （或者用更复杂的算法融合，比如把各基分类器的输出作为特征，使用逻辑回归作为融合模型进行最后的结果预测）作为最终的输出。

- 集成学习的例子：
    1. Adaboost-自适应Boosting（Adaptive Boosting)
    其基分类器的训练和合并的基本步骤如下：
        1. 确定基分类器：这里可以选取ID3决策树作为基分类器。
        事实上，任何分类模型都可以作为基分类器，但树形模型由于结构简单且较易产生随机性所以比较常用。
        2. 训练基分类器：假设训练集为$\lbrace x_i, y_i \rbrace, i=1,...,M$，其中$y_i \in \lbrace -1, 1 \rbrace$，
        并且有$T$个基分类器，则可以按照如下过程来训练基分类器：
            - 初始化采样分布 $ D_1(i) = \frac {1}{M} $；
            - 令$ t = 1,2,...,T $循环：
                - 从训练集中，按照$D_t$分布，采样出子集$S_t = \lbrace x_i, y_i \rbrace, i=1,...,M_t $；
                - 用$S_t$训练出基分类器 $h_t$；
                - 计算$h_t$的错误率：
                $ \epsilon_t = \frac{\sum_{i=1}^{M_t} \: I[h_t(x_i) \neq y_i]D_t(x_i)}{M_t}$
                其中$I[]$为判别函数；
                - 计算基分类器$h_t$权重 $\alpha_t = \log \frac{1 - \epsilon_t}{\epsilon_t}$；
                - 设置下一次采样
                $$ D_{t+1} = \begin{cases} D_t(i)或者\frac{D_t(i)(1 - \epsilon_t)}{\epsilon_t}, & h_t(x_i) \neq y_i; \quad \frac{D_t(i) \epsilon_t}{(1 - \epsilon_t)}, & h_t(x_i) = y_i \end{cases} $$
                并将它归一化为一个概率分布函数。
        3. 合并基分类器：给定一个位置样本$z$，输出分类结果为加权投票的结果$Sign(\sum_{t=1}^T \: h_t(z) \alpha_t)$。
        
        从Adaboost的例子中可以明显地看到Boosting的思想，对分类正确的样本降低了权重，对分类错误的样本升高或者保持权重不变。
        在最后进行模型融合的过程中，也根据错误率对基分类器进行加权融合。错误率低的分类器拥有更大的“话语权”。
        
    2. GBDT-梯度提升决策树（Gradient Boosting Decision Tree）
        - 核心思想：每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。
        - 这里使用残差继续学习，就是GBDT中Gradient Boosted所表达的意思。

## 03 基分类器
基分类器的选择是集成学习主要步骤中的第一步，也是非常重要的一步。
到底选择什么样的基分类器，为什么很多集成学习模型都选择决策树作为基分类器，这些都是需要明确的问题，
做到知其然，也知其所以然。

### 常用的基分类器是什么？
最常用的基分类器是决策树，主要有以下3个方面的原因：
1. 决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重。
2. 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。
3. 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，
这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，
很好地引入了随机性。

除了决策树外，神经网络模型也适合作为基分类器，主要由于神经网络模型也比较“不稳定”，
而且还可以通过调整神经元数量、连接方式、网络层数、初始权值等方式引入随机性。

### 可否将随机森林中的基分类器，由据测算替换为线性分类器或K-NN？请解释为什么？
随机森林属于Bagging类的集成学习。Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小。
Bagging所采用的基分类器，最好是本身对样本分布较为敏感的（即所谓不稳定的分类器），这样Bagging才能有用武之地。
线性分类器或者K-近邻都是较为稳定的分类器，本身方差就不大，
所以以它们为基分类器使用Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，
而导致他们在训练中更难收敛，从而增大了集成分类器的偏差。

## 04 偏差与方差
经常用过拟合、欠拟合来定性地描述模型是否很好地解决了特定的问题。从定量的角度来说，
可以用模型的偏差（Bias）与方差（Variance）来描述模型的性能。集成学习往往能够“神奇”地提升弱分类器的性能。

### 什么是偏差和方差
在有监督学习中，模型的泛化误差来源于两个方面——偏差和方差。
- 偏差 <br>
偏差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差。
偏差通常是由于对学习算法做了错误的假设所导致的，比如真实模型是某个二次函数，但假设模型是一次函数。
由偏差带来的误差通常在训练误差上就能体现出来。

- 方差 <br>
方差指的是由所有采样得到的大小为m的训练数据集训练出的所有模型的输出的方差。
方差通常是由于模型的复杂度相对于训练样本数m过高导致的，比如一共有100个训练样本，
而假设模型是阶数不大于200的多项式函数。由方差带来的误差通常体现在测试误差相对于训练误差的增量上。

偏差和方差示意图
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_02.png?raw=true)

### 如何从减小方差和偏差的角度解释Boosting和Bagging的原理？
简单回答这个问题就是：Bagging能够提高弱分类器性能的原因是降低了方差，Boosting能够提升弱分类器性能的原因是降低了偏差。

- Bagging是Bootstrap Aggregating的简称，意思是再抽样，然后在每个样本上训练出来的模型取平均。
    - 假设有$n$个随机变量，方差即为$\sigma^2$，在随机变量完全独立的情况下，$n$个随机变量的方差为$\frac{\sigma^2}{n}$，
    也就是说方差减小到了原来的$\frac{1}{n}$。
    - 再从模型的角度看，对$n$个独立不相关的模型的预测结果取平均，方差是原来单个模型的$\frac{1}{n}$。
    这个描述不甚严谨，但原理已经讲得很清楚了。当然，模型之间不可能完全独立。为了追求模型的独立性，
    诸多Bagging的方法做了不同的改进。比如在随机森林算法中，每次选取节点分裂属性时，会随机抽取一个属性子集，
    而不是从所有属性中选取最优属性，这就是为了避免弱分类器之间过强的相关性。
    通过训练集的重采样也能够带来弱分类器之间的一定独立性，从而降低Bagging后模型的方差。
    
- Boosting <br>
Boosting的训练过程：在训练好一个弱分类器后，需要计算弱分类器的错误或者残差，作为下一个分类器的输入。
这个过程本身就是在不断减小损失函数，来使模型不断逼近“靶心”，使得模型偏差不断降低。
但Boosting的过程并不会显著降低方差。这是因为Boosting的训练过程使得各弱分类器之间是强相关的，
缺乏独立性，所以并不会对降低方差有作用。

方差和偏差是相辅相成，矛盾又统一的，二者并不能完全独立的存在。对于给定的学习任务和训练数据集，
需要对模型的复杂度做合理的假设。如果模型复杂度过低，虽然方差很小，但是偏差会很高；
如果模型复杂度过高，虽然偏差降低了，但是方差会很高。所以需要综合考虑偏差和方差选择合适复杂度的模型进行训练。

如图所示，泛化误差、偏差、方差和模型复杂度的关系。
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_03.png?raw=true)

## 05 梯度提升决策树的基本原理
梯度提升决策树（Gradient Boosting Decision Tree，GBDT）是Boosting算法中非常流行的模型，
也是近来在机器学习竞赛、商业应用中表现都非常优秀的模型。GBDT非常好地体现了“从错误中学习”的理念，
基于决策树预测的残差进行迭代的学习。

### GBDT的基本原理是什么？
> [参考：理论理解](https://blog.csdn.net/puqutogether/article/details/41957089) <br>
> [参考：基于残差的版本](https://blog.csdn.net/puqutogether/article/details/44752611) <br>
> [参考：基于梯度的版本](https://blog.csdn.net/puqutogether/article/details/44781035) <br>

Boosting和Bagging是两大集成学习的框架。
相比于Bagging中各个弱分类器可以独立地进行训练，Boosting中的弱分类器需要依次生成。
在每一轮迭代中，基于已生成的弱分类器集合（即当前模型）的预测结果，新的弱分类器会重点关注那些还没有被正确预测的样本。

Gradient Boosting是Boosting中的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，
然后将训练好的弱分类器以累加的形式结合到现有模型中。

Gradient Boosting算法的基本流程，在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，
然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

采用决策树作为弱分类器的Gradient Boosting算法被称为GBDT，有时又被称为MART（Multiple Additive Regression Tree）。
GBDT中使用的决策树通常为CART。

由于GBDT是利用残差训练的，在预测的过程中，也需要把所有树的预测值加起来，得到最终的预测结果。

GBDT算法的伪代码如下: <br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_04.png?raw=true)

### 梯度提升和梯度下降的区别和联系是什么？
两者都是在每一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更新，只不过在梯度下降中，
模型是以参数化形式表示，从而模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，
而是直接定义在函数空间中，从而大大扩展了可以使用的模型种类。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/ensemble_05.png?raw=true)

### GBDT的有点和局限性有哪些？
- 优点：
    - 预测阶段的计算速度快，树与树之间可并行化计算。
    - 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
    - 采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系，
    并且也不需要对数据进行特殊的预处理如归一化等。

- 局限性：
    - GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
    - GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
    - 训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。
    
## 06 XGBoost与GBDT的联系和区别
XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，
被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。
在使用XGBoost平台的时候，也需要熟悉XGBoost平台的内部实现和原理，
这样才能够更好地进行模型调参并针对特定业务场景进行模型改进。

### XGBoost与GBDT的联系和区别有哪些？
