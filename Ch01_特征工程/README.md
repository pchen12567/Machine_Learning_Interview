# 第一章 特征工程
- 特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。
- 从本质上来讲，特征工程是一个表示和展现数据的过程。
- 在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

两种常用的数据类型
- 结构化数据
    - 结构化数据类型可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型；
    - 每一行数据表示一个样本的信息。
- 非结构化数据
    - 非结构化数据主要包括文本、图像、音频、视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。
----
## 01 特征归一化 Normalization
- 为了消除数据特征之间的量纲影响，需要对特征进行归一化处理，使得不同指标之间具有可比性。
- 要想得到更为准确的结果，需要进行特征归一化处理，使各指标处于同一数值量级，以便进行分析。

### 为什么需要对数值类型的特征做归一化？
对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内
1. 线性函数归一化 Min-Max Scaling <br>
它对原始数据进行线性变换，使结果映射到[0,1]的范围，实现对原始数据的等比缩放，其公式如下: <br>
$$ X_{norm} = \cfrac{X - X_{min}}{X_{max} - X_{min}} $$

2. 零均值归一化 Z-Score Normalization <br>
它会将原始数据映射到均值为0、标准差为1的分布上。假设原始特征的均值为$\mu$、标准差为$\sigma$，那么其公式为: <br>
$$ z = \cfrac{x - \mu}{\sigma} $$

**数据归一化并不是万能的。** <br>
在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。<br>
但对于决策树模型则并不适用，因为信息增益比跟特征是否经过归一化是无关的，归一化并不会改变样本在特征X上的信息增益。

## 02 类别型特征 Categorical Feature
- 类别型特征主要是指性别、血型等只在有限选项内取值的特征。
- 类别型特征原始输入通常是字符串形式，除了决策树等少数模型外，类别型特性必须经过处理转换成数值型特征才能正确工作。

### 在对数据进行预处理时，应该怎样处理类别型特征？
1. 序号编码 <br>
通常用于处理类别间具有大小关系的数据，例如成绩可以分为低、中、高三档。

2. 独热编码 <br>
通常用于处理类别间不具有大小关系的特征，例如血型。
    - 使用稀疏向量来节省空间。
    - 配合特征选择来降低维度。
    
3. 二进制编码 <br>
    - 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别，然后将类别ID对应的二进制编码作为结果。
    - 本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数小于独热编码，节省了存储空间。
![](https://github.com/pchen12567/picture_store/blob/master/Interview/feature_01.jpg?raw=true)

### 分类和回归问题的主要区别是什么？
1. 输出数据的类型
    - 分类输出的数据类型是离散型数据，也就是分类的标签。<br>
    比如通过学生学习情况预测考试是否通过，这里的预测结果是考试通过或者不通过，这2种离散数据。
    - 回归输出的数据类型是连续型数据。<br>
    比如通过学习时间预测学生的考试分数，这里的预测结果分数，是连续型数据。

2. 想通过机器学习算法得到什么
    - 分类算法得到的是一个决策面，用于对数据集中的数据进行分类。
    - 回归算法得到的是一个最优拟合线，这条线可以最好的接近数据集中的各个点。

3. 对模型的评估标准
    - 在分类算法中，通常会使用正确率作为指标，也就是预测结果中分类正确数据占总数据的比例。
    - 在回归算法中，用决定系数R平方来评估模型的好坏。R平方表示有多少百分比的Y波动被回归线描述。

## 03 高维组合特征的处理
### 什么是组合特征？如何处理高维组合特征？
为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

对于高维度组合特征 <br>
以用户ID和物品ID对点击的影响为例，若用户的数量为m，物品的数量为n，那么需要学习的参数规模为$ m * n $。<br>
一种行之有效的方法是将特征分别使用K维的低维向量表示，需要学习的参数的规模变为 $ m * k + n * k $，这其实等价于矩阵分解。

## 04 组合特征
在很多实际问题中，常常需要面对多种高维特征。如果简单地两两组合，依然容易存在参数过多、过拟合等问题，
而且并不是所有的特征组合都是有意义的。<br>
因此，需要一种有效的方法来找到应该对哪些特征进行组合。

### 怎样有效地找到组合特征？
(To be continue...)

## 05 文本表示模型
### 有哪些文本表示模型？它们各有什么优缺点？
> [Language Model 参考1](https://github.com/pchen12567/AI_For_NLP/blob/master/Week_02_LanguageModel/LectureCode_02.ipynb)
>
> [Language Model 参考2](https://github.com/pchen12567/AI_For_NLP/blob/master/Week_02_LanguageModel/Assignment_02.ipynb)
>
> [TF-IDF 参考](https://github.com/pchen12567/AI_For_NLP/blob/master/Week_06_TFIDF/LectureCode_06.ipynb)
1. 词袋模型和 N_gram 模型
    - 最基础的文本表示模型就是词袋模型，就是将每篇文章看成一袋子词，并且忽略每个词出现的顺序。
    - 具体说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词。
    - 每一个维度对应的权重反映了这个词在原文章中的重要程度，常用TF-IDF来计算权重。
$$ TF-IDF_{(t,d)} = TF_{(t,d)} * IDF_{(t)} $$ 
    - TF为单词$t$在文档$d$中出现的频率，IDF(t)是逆文档频率，用来衡量单词$t$对表达语义所起的重要性，其公式如下：
$$ TF_{(t,d)} = \frac{n_{(t,d)}}{\sum_{1}^{k}n_{(k,d)}} $$
$$ IDF_{t} = log_{10}\frac{N}{df_{t} + 1} $$
        - $n_{(t,d)}$: 单词$t$在文档$d$中的频率。
        - $\sum_1^k n_{(k,d)}$: 文档$d$中所有单词的频率之和。
        - $N$: 文档总数。
        - $df_t$: 包含单词$t$的文档数量。
    - 直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，
    对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。
    - 但是将文章进行单词级别的划分有时候并不是一种好的做法，
    通常，可以将连续出现的$n$个词（n <= N）组成的词组（N_gram）也作为一个单独的特征放到向量表示中去，构成N_gram模型。
    - 同一个词可能有多个词性变化却具有相似含义，在实际应用中，一般会对单词进行词干抽取(Word Stemming)处理，
    即将不同词性的词统一称为统一词干的形式。
2. 主题模型 <br>
主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。
3. 词嵌入与深度学习模型
    - 词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。
    K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。
    - 由于词嵌入将每个词映射成一个K维的向量，如果一篇文档有N个词，就可以用一个N×K维的矩阵来表示这篇文档，
    但是这样的表示过于底层。在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。
    因此，还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。
    而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。
    从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。
    卷积神经网络和循环神经网络的结构在文本表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。
    与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，
    并且降低了过拟合的风险。

## 06 Word2Vec
谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。
Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别是CBOW（Continues Bag of Words）和Skip-gram。

### Word2Vec 是如何工作的？它和LDA有什么区别和联系？
1. Word2Vec
    - CBOW 的目标是根据上下文出现的词语来预测当前词的生成概率
    - Skip-gram 是根据当前词来预测上下文中各词的生成概率
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/feature_02.jpg?raw=true)
    - CBOW和Skip-gram都可以表示成由输入层（Input）、映射层（Projection）和输出层（Output）组成的神经网络。
    - 输入层中的每个词由独热编码方式表示，即所有词均表示成一个N维向量，其中N为词汇表中单词的总数。
    在向量中，每个词都将与之对应的维度置为1，其余维度的值均设为0。
    - 在映射层（又称隐含层）中，K个隐含单元（Hidden Units）的取值可以由N维输入向量以及连接输入和隐含单元之间的N×K维权重矩阵计算得到。
    在CBOW中，还需要将各个输入词所计算出的隐含单元求和。
    - 输出层向量的值可以通过隐含层向量（K维），以及连接隐含层和输出层之间的K×N维权重矩阵计算得到。
    输出层也是一个N维向量，每维与词汇表中的一个单词相对应。
    最后，对输出层向量应用Softmax激活函数，可以计算出每个单词的生成概率。Softmax激活函数的定义为
    $$ P(y = w_n|x) = \frac{e^{x_n}}{\sum_{k=1}^N e^{x_k}} $$
    其中$x$代表$N$维的原始输出向量，$x_n$为在原始输出向量中，与单词$w_n$所对应维度的取值。
    - 接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化。
    从输入层到隐含层需要一个维度为N×K的权重矩阵，从隐含层到输出层又需要一个维度为K×N的权重矩阵，
    学习权重可以用反向传播算法实现，每次迭代时将权重沿梯度更优的方向进行一小步更新。
    但是由于Softmax激活函数中存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得每次迭代过程非常缓慢，
    由此产生了Hierarchical Softmax和Negative Sampling两种改进方法。
    训练得到维度为N×K和K×N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。
2. 与LDA的区别和联系
    - LDA是利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布。
    而Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的词向量表示更多地融入了上下文共现的特征。
    也就是说，如果两个单词所对应的Word2Vec向量相似度较高，那么它们很可能经常在同样的上下文中出现。
    - 主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，
    其中包括需要推测的隐含变量（即主题）；而词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，
    需要通过学习网络的权重以得到单词的稠密向量表示。

## 07 图像数据不足时的处理方法
在机器学习中，绝大部分模型都需要大量的数据进行训练和学习（包括有监督学习和无监督学习），然而在实际应用中经常会遇到训练数据不足的问题。
比如图像分类，作为计算机视觉最基本的任务之一，其目标是将每幅图像划分到指定类别集合中的一个或多个类别中。

### 在图像分类任务中，训练数据不足会带来什么问题？如何缓解数据量不足带来的问题？
(To be continue...)