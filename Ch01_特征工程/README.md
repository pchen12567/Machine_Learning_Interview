# 第一章 特征工程
- 特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。
- 从本质上来讲，特征工程是一个表示和展现数据的过程。
- 在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

两种常用的数据类型
- 结构化数据
    - 结构化数据类型可以看作关系型数据库的一张表，每列都有清晰的定义，包含了数值型、类别型两种基本类型；
    - 每一行数据表示一个样本的信息。
- 非结构化数据
    - 非结构化数据主要包括文本、图像、音频、视频数据，其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数据的大小各不相同。
----
## 01 特征归一化 Normalization
- 为了消除数据特征之间的量纲影响，需要对特征进行归一化处理，使得不同指标之间具有可比性。
- 要想得到更为准确的结果，需要进行特征归一化处理，使各指标处于同一数值量级，以便进行分析。

### 为什么需要对数值类型的特征做归一化？
对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值区间内
1. 线性函数归一化 Min-Max Scaling <br>
它对原始数据进行线性变换，使结果映射到[0,1]的范围，实现对原始数据的等比缩放，其公式如下: <br>
$$ X_{norm} = \cfrac{X - X_{min}}{X_{max} - X_{min}} $$

2. 零均值归一化 Z-Score Normalization <br>
它会将原始数据映射到均值为0、标准差为1的分布上。假设原始特征的均值为$\mu$、标准差为$\sigma$，那么其公式为: <br>
$$ z = \cfrac{x - \mu}{\sigma} $$

**数据归一化并不是万能的。** <br>
在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。<br>
但对于决策树模型则并不适用，因为信息增益比跟特征是否经过归一化是无关的，归一化并不会改变样本在特征X上的信息增益。

## 02 类别型特征 Categorical Feature
- 类别型特征主要是指性别、血型等只在有限选项内取值的特征。
- 类别型特征原始输入通常是字符串形式，除了决策树等少数模型外，类别型特性必须经过处理转换成数值型特征才能正确工作。

### 在对数据进行预处理时，应该怎样处理类别型特征？
1. 序号编码 <br>
通常用于处理类别间具有大小关系的数据，例如成绩可以分为低、中、高三档。

2. 独热编码 <br>
通常用于处理类别间不具有大小关系的特征，例如血型。
    - 使用稀疏向量来节省空间。
    - 配合特征选择来降低维度。
    
3. 二进制编码 <br>
    - 二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别，然后将类别ID对应的二进制编码作为结果。
    - 本质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量，且维数小于独热编码，节省了存储空间。
![](https://github.com/pchen12567/picture_store/blob/master/Interview/feature_01.jpg?raw=true)

## 03 高维组合特征的处理
### 什么是组合特征？如何处理高维组合特征？
为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。

对于高维度组合特征 <br>
以用户ID和物品ID对点击的影响为例，若用户的数量为m，物品的数量为n，那么需要学习的参数规模为$ m * n $ <br>
一种行之有效的方法是将特征分别使用K维的低维向量表示，需要学习的参数的规模变为 $ m * k + n * k $，这其实等价于矩阵分解。

## 04 组合特征
在很多实际问题中，常常需要面对多种高维特征。如果简单地两两组合，依然容易存在参数过多、过拟合等问题，
而且并不是所有的特征组合都是有意义的。<br>
因此，需要一种有效的方法来找到应该对哪些特征进行组合。

### 怎样有效地找到组合特征？

## 05 文本表示模型
### 有哪些文本表示模型？它们各有什么优缺点？
1. 词袋模型和 N_gram 模型
    - 最基础的文本表示模型就是词袋模型，就是将每篇文章看成一袋子词，并且忽略每个词出现的顺序。
    - 具体说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词。
    - 每一个维度对应的权重反映了这个词在原文章中的重要程度，常用TF-IDF来计算权重。
$$ TF-IDF_{(t,d)} = TF_{(t,d)} * IDF_{(t)} $$ 
    - TF为单词$t$在文档$d$中出现的频率，IDF(t)是逆文档频率，用来衡量单词$t$对表达语义所起的重要性，其公式如下：
$$ TF_{(t,d)} = \frac{n_{(t,d)}}{\sum_{1}^{k}n_{(k,d)}} $$
$$ IDF_{t} = log_{10}\frac{N}{df_{t} + 1} $$
    - 直观的解释是，如果一个单词在非常多的文章里面都出现，那么它可能是一个比较通用的词汇，
    对于区分某篇文章特殊语义的贡献较小，因此对权重做一定惩罚。
    - 但是将文章进行单词级别的划分有时候并不是一种好的做法，
    通常，可以将连续出现的$n$个词（n <= N）组成的词组（N_gram）也作为一个单独的特征放到向量表示中去，构成N_gram模型。
    - 同一个词可能有多个词性变化却具有相似含义，在实际应用中，一般会对单词进行词干抽取(Word Stemming)处理，
    即将不同词性的词统一称为统一词干的形式。
2. 主题模型 <br>
主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布特性），并且能够计算出每篇文章的主题分布。
3. 词嵌入与深度学习模型
    - 词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。
    K维空间的每一维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。
    - 由于词嵌入将每个词映射成一个K维的向量，如果一篇文档有N个词，就可以用一个N×K维的矩阵来表示这篇文档，
    但是这样的表示过于底层。在实际应用中，如果仅仅把这个矩阵作为原文本的表示特征输入到机器学习模型中，通常很难得到令人满意的结果。
    因此，还需要在此基础之上加工出更高层的特征。在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升。
    而深度学习模型正好为我们提供了一种自动地进行特征工程的方式，模型中的每个隐层都可以认为对应着不同抽象层次的特征。
    从这个角度来讲，深度学习模型能够打败浅层模型也就顺理成章了。
    卷积神经网络和循环神经网络的结构在文本表示中取得了很好的效果，主要是由于它们能够更好地对文本进行建模，抽取出一些高层的语义特征。
    与全连接的网络结构相比，卷积神经网络和循环神经网络一方面很好地抓住了文本的特性，另一方面又减少了网络中待学习的参数，提高了训练速度，
    并且降低了过拟合的风险。

## 06 Word2Vec











