# 第五章 非监督学习
- 在实际工作中，经常会遇到这样一类问题：给机器输入大量的特征数据，并期望机器通过学习找到数据中存在的某种共性特征或者结构，
亦或是数据之间存在的某种关联。例如，视频网站根据用户的观看行为对用户进行分组从而建立不同的推荐策略，
或是寻找视频播放是否流畅与用户是否退订之间的关系等。这类问题被称作“非监督学习”问题，它并不是像监督学习那样希望预测某种输出结果。
- 相比于监督学习，非监督学习的输入数据没有标签信息，需要通过算法模型来挖掘数据内在的结构和模式。非监督学习主要包含两大类学习方法：
数据聚类和特征变量关联。其中，聚类算法往往是通过多次迭代来找到数据的最优分割，而特征变量关联则是利用各种相关性分析方法来找到变量之间的关系。

## 01 K均值聚类
支持向量机、逻辑回归、决策树等经典的机器学习算法主要用于分类问题，即根据一些已给定类别的样本，训练某种分类器，
使得它能够对类别未知的样本进行分类。与分类问题不同，聚类是在事先并不知道任何样本类别标签的情况下，
通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_01.png?raw=true)

分类问题属于监督学习的范畴，而聚类则是非监督学习。K均值聚类（K-Means Clustering）是最基础和最常用的聚类算法。它的基本思想是，
通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。
特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和：
$$ J(c,\mu) = \sum_{i=1}^M || x_i - \mu_{c_i} ||^2 $$
其中$x_i$代表第$i$个样本，$c_i$是$x_i$所属的簇，$\mu_{c_i}$代表簇对应的中心点，$M$是样本总数。

### 简述K均值算法的具体步骤
K均值聚类的核心目标是将给定的数据集划分成$K$个簇，并给出每个数据对应的簇中心点。算法的具体步骤描述如下：
1. 数据预处理，如归一化、离群点处理等。
2. 随机选取$K$个簇中心，记为$\mu_1^{(0)}, \mu_2^{(0)},..., \mu_K^{(0)}$。
3. 定义损失函数：$ J(c, \mu) = \min_{\mu} \min_{c} \sum_{i=1}^M ||x_i - \mu_{c_i}||^2 $。
4. 令$ t=0,1,2,...$为迭代步数，重复下面的过程直到$J$收敛：
    - 对于每一个样本$x_i$，将其分配到距离最近的簇：
    $$ c_i^{(t)} \leftarrow arg \min_{k} ||x_i - \mu_k^{t}||^2 $$
    - 对于每一个类簇$k$，重新计算该类簇的中心:
    $$ \mu_k^{(t+1)} \leftarrow arg \min_{k} \sum_{i:c_i^{(t)} = k} ||x_i - \mu ||^2 $$
    
K均值算法在迭代时，假设当前$J$没有达到最小值，那么首先固定簇中心$\lbrace \mu_k \rbrace$，调整每个样例$x_i$所属的类别$c_i$来让$J$函数减少；
然后固定$\lbrace c_i \rbrace$，调整簇中心$\lbrace \mu_k \rbrace$使$J$减小。这两个过程交替循环，$J$单调递减：当$J$递减到最小值时，
$\lbrace \mu_k \rbrace$和$\lbrace c_i \rbrace$也同时收敛。

K_means算法的迭代过程如图所示：首先，给定二维空间上的一些样本点，直观上这些点可以被分成两类；
接下来，初始化两个中心点（棕色和黄色叉子代表中心点），并根据中心点的位置计算每个样本所属的簇；
然后根据每个簇中的所有点的平均值计算新的中心点位置；图（e）和图（f）展示了新一轮的迭代结果；在经过两轮的迭代之后，算法基本收敛。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_02.png?raw=true)

### K均值算法的优缺点是什么？如何对其进行调优？
K均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、
无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类等。但是瑕不掩瑜，
K均值聚类的优点也是很明显和突出的，主要体现在：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是$O(NKt)$接近于线性，
其中$N$是数据对象的数目，$K$是聚类的簇数，$t$是迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。

K均值算法的调优一般可以从以下几个角度出发。
1. 数据归一化和离群点处理。<br>
K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，
所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，
因此使用K均值聚类算法之前通常需要对数据做预处理。

2. 合理选择K值。<br>
K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，
或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，可以尝试不同的K值，
并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数，如图所示：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_03.png?raw=true)<br>
由图可见，$K$值越大，距离和越小；并且，当$K=3$时，存在一个拐点，就像人的肘部一样；当$K \in (1,3)$时，曲线急速下降；
当$K>3$时，曲线趋于平稳。手肘法认为拐点就是$K$的最佳值。

    手肘法是一个经验方法，缺点就是不够自动化，因此又提出了一些更先进的方法，其中包括比较有名的Gap Statistic方法。
    Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的Gap statistic所对应的$K$即可，因此该方法也适用于批量化作业。
    在这里继续使用上面的损失函数，当分为$K$簇时，对应的损失函数记为$D_k$。Gap Statistic定义为：
    $$ Gap(K) = E(\log{D_k}) - \log{D_k} $$
    其中$E(\log{D_k})$是$\log{D_k}$的期望，一般通过蒙特卡洛模拟产生。
    在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做$K$均值，得到一个$D_k$；
    重复多次就可以计算出$E(\log{D_k})$的近似值。那么Gap(K)有什么物理含义呢？它可以视为随机样本的损失与实际样本的损失之差。
    试想实际样本对应的最佳簇数为$K$，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，
    从而Gap(K)取得最大值所对应的K值就是最佳的簇数。根据上式计算$K =1,2,...,9$所对应的Gap Statistic，如图所示：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_04.png?raw=true)<br>
    由图可见，当K=3时，Gap(K)取值最大，所以最佳的簇数是K=3。

3. 采用核函数。<br>
采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，
并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，
这时算法又称为核K均值算法，是核聚类方法的一种。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，
并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。

### 针对K均值算法的缺点，有哪些改进的模型？
- K均值算法的主要缺点如下。
    1. 需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。
    2. K均值只能收敛到局部最优，效果受到初始值很大。
    3. 易受到噪点的影响。
    4. 样本点只能被划分到单一的类中。
    
- K-means++ 算法 <br>
K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法中，最具影响力的当属K-means++算法。
原始K均值算法最开始随机选取数据集中$K$个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心。
假设已经选取了$n$个初始聚类中心$(0<n<K)$，则在选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。
在选取第一个聚类中心$(n=1)$时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。
当选择完初始点后，K-means++后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点。

- ISODATA算法 <br>
当K值的大小不确定时，可以使用ISODATA算法。ISODATA的全称是迭代自组织数据分析法。在K均值算法中，聚类个数K的值需要预先人为地确定，
并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA算法就是针对这个问题进行了改进，
它的思想也很直观。当属于某个类别的样本数过少时，把该类别去除；当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。
ISODATA算法在K均值算法的基础之上增加了两个操作，一是分裂操作，对应着增加聚类中心数；二是合并操作，对应着减少聚类中心数。
ISODATA算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量$K_o$，还需要制定3个阈值。
下面介绍ISODATA算法的各个输入参数。
    1. 预期的聚类中心数目$K_o$。在ISODATA运行过程中聚类中心数可以变化，$K_o$是一个用户指定的参考值，
    该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从$K_o$的一半，到两倍$K_o$。
    2. 每个类所要求的最少样本数目$N_{\min}$。如果分裂后会导致某个子类别所包含样本数目小于该阈值，就不会对该类别进行分裂操作。
    3. 最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足(i)，进行分裂操作。
    4. 两个聚类中心之间所允许最小距离$D_{\min}$。如果两个类靠得非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，
    则对这两个类进行合并操作。

    如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。
    
### 证明K均值算法的收敛性
首先，需要知道K均值聚类的迭代算法实际上是一种最大期望算法（Expectation-Maximization algorithm），简称EM算法。
EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。假设有m个观察样本，模型的参数为θ，
最大化对数似然函数可以写成如下形式：
$$ \theta = arg \max_{\theta} \sum_{i=1}^m \log{P(x^{ (i)}| \theta )} $$

(To be continue...)

## 高斯混合模型
高斯混合模型（Gaussian Mixed Model，GMM）也是一种常见的聚类算法，与K均值算法类似，同样使用了EM算法进行迭代计算。
高斯混合模型假设每个簇的数据都是符合高斯分布（又叫正态分布）的，当前数据呈现的分布就是各个簇的高斯分布叠加在一起的结果。
图5.6是一个数据分布的样例，如果只用一个高斯分布来拟合图中的数据，图中所示的椭圆即为高斯分布的二倍标准差所对应的椭圆。
直观来说，图中的数据明显分为两簇，因此只用一个高斯分布来拟和是不太合理的，需要推广到用多个高斯分布的叠加来对数据进行拟合。
图5.7是用两个高斯分布的叠加来拟合得到的结果。这就引出了高斯混合模型，即用多个高斯分布函数的线形组合来对数据分布进行拟合。
理论上，高斯混合模型可以拟合出任意类型的分布。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_05.png?raw=true)
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_06.png?raw=true)

### 高斯混合模型的核心思想是什么？它是如何迭代计算的？
说起高斯分布，通常身高、分数等都大致符合高斯分布。因此，当研究各类数据时，假设同一类的数据符合高斯分布，也是很简单自然的假设；
当数据事实上有多个类，或者希望将数据划分为一些簇时，可以假设不同簇中的样本各自服从不同的高斯分布，由此得到的聚类算法称为高斯混合模型。

高斯混合模型的核心思想是，假设数据可以看作从多个高斯分布中生成出来的。在该假设下，每个单独的分模型都是标准高斯模型，
其均值$\mu_i$和方差$\sum_i$是待估计的参数。此外，每个分模型都还有一个参数$\pi_j$，可以理解为权重或生成数据的概率。
高斯混合模型的公式为：
$$ p(x) = \sum_{i=1}^K \pi_i N (x | \mu_i, \sum_i) $$
高斯混合模型是一个生成式模型。可以这样理解数据的生成过程，假设一个最简单的情况，即只有两个一维标准高斯分布的分模型$N(0,1)$和$N(5,1)$，
其权重分别为0.7和0.3。那么，在生成第一个数据点时，先按照权重的比例，随机选择一个分布，比如选择第一个高斯分布，接着从$N(0,1)$中生成一个点，
如−0.5，便是第一个数据点。在生成第二个数据点时，随机选择到第二个高斯分布$N(5,1)$，生成了第二个点4.7。如此循环执行，便生成出了所有的数据点。

然而，通常并不能直接得到高斯混合模型的参数，而是观察到了一系列数据点，给出一个类别的数量K后，希望求得最佳的K个高斯分模型。
因此，高斯混合模型的计算，便成了最佳的均值$\mu$，方差$\sum$、权重$\pi$的寻找，这类问题通常通过最大似然估计来求解。
遗憾的是，此问题中直接使用最大似然估计，得到的是一个复杂的非凸函数，目标函数是和的对数，难以展开和对其求偏导。

在这种情况下，可以用上面已经介绍过的EM算法框架来求解该优化问题。EM算法是在最大化目标函数时，先固定一个变量使整体函数变为凸优化函数，
求导得到最值，然后利用最优参数更新被固定的变量，进入下一个循环。具体到高斯混合模型的求解，EM算法的迭代过程如下。

首先，初始随机选择各参数的值。然后，重复下述两步，直到收敛。
1. E步骤。根据当前的参数，计算每个点由某个分模型生成的概率。
2. M步骤。使用E步骤估计出的概率，来改进每个分模型的均值，方差和权重。

也就是说，并不知道最佳的K个高斯分布的各自3个参数，也不知道每个数据点究竟是哪个高斯分布生成的。所以每次循环时，先固定当前的高斯分布不变，
获得每个数据点由各个高斯分布生成的概率。然后固定该生成概率不变，根据数据点和生成概率，获得一个组更佳的高斯分布。
循环往复，直到参数的不再变化，或者变化非常小时，便得到了比较合理的一组高斯分布。

高斯混合模型与K均值算法的相同点是，它们都是可用于聚类的算法；都需要指定K值；都是使用EM算法来求解；都往往只能收敛于局部最优。
而它相比于K均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；
并且可以用于生成新的样本点。

## 03 自组织映射神经网络
自组织映射神经网络（Self-Organizing Map，SOM）是无监督学习方法中一类重要方法，可以用作聚类、高维可视化、数据压缩、特征提取等多种用途。
在深度神经网络大为流行的今天，谈及自组织映射神经网络依然是一件非常有意义的事情，这主要是由于自组织映射神经网络融入了大量人脑神经元的信号处理机制，
有着独特的结构特点。该模型由芬兰赫尔辛基大学教授Teuvo Kohonen于1981年提出，因此也被称为Kohonen网络。

### 自组织映射神经网络是如何工作的？它与K均值算法有何区别？
(To be continue...)

## 04 聚类算法的评估
人具有很强的归纳思考能力，善于从一大堆碎片化的事实或者数据中寻找普遍规律，并得到具有逻辑性的结论。以用户观看视频的行为为例，
可以存在多种直观的归纳方式，比如从喜欢观看内容的角度，可以分为动画片、偶像剧、科幻片等；从常使用的设备角度，
可以分为台式电脑、手机、平板便携式设备、电视等；从使用时间段上看，有傍晚、中午、每天、只在周末观看的用户，等等。
对所有用户进行有效的分组对于理解用户并推荐给用户合适的内容是很重要的。通常这类问题没有观测数据的标签或者分组信息，
需要通过算法模型来寻求数据内在的结构和模式。

### 以聚类问题为例，假设没有外部标签数据，如何评估两个聚类算法的优劣？
上面描述中的例子就是一个典型的聚类问题，从中可以看出，数据的聚类依赖于实际需求，同时也依赖于数据的特征度量以及评估数据相似性的方法。
相比于监督学习，非监督学习通常没有标注数据，模型、算法的设计直接影响最终的输出和模型的性能。为了评估不同聚类算法的性能优劣，
需要了解常见的数据簇的特点。
- 以中心定义的数据簇：<br>
这类数据集合倾向于球形分布，通常中心被定义为质心，即此数据簇中所有点的平均值。集合中的数据到中心的距离相比到其他簇中心的距离更近。
- 以密度定义的数据簇：<br>
这类数据集合呈现和周围数据簇明显不同的密度，或稠密或稀疏。当数据簇不规则或互相盘绕，并且有噪声和离群点时，常常使用基于密度的簇定义。
- 以连通定义的数据簇：<br>
这类数据集合中的数据点和数据点之间有连接关系，整个数据簇表现为图结构。该定义对不规则形状或者缠绕的数据簇有效。
- 以概念定义的数据簇：<br>
这类数据集合中的所有数据点具有某种共同性质。

由于数据以及需求的多样性，没有一种算法能够适用于所有的数据类型、数据簇或应用场景，似乎每种情况都可能需要一种不同的评估方法或度量标准。
例如，K均值聚类可以用误差平方和来评估，但是基于密度的数据簇可能不是球形，误差平方和则会失效。在许多情况下，
判断聚类算法结果的好坏强烈依赖于主观解释。尽管如此，聚类算法的评估还是必需的，它是聚类分析中十分重要的部分之一。

聚类评估的任务是估计在数据集上进行聚类的可行性，以及聚类方法产生结果的质量。这一过程又分为三个子任务。
1. 估计聚类趋势。<br>
这一步骤是检测数据分布中是否存在非随机的簇结构。如果数据是基本随机的，那么聚类的结果也是毫无意义的。
可以观察聚类误差是否随聚类类别数量的增加而单调变化，如果数据是基本随机的，即不存在非随机簇结构，
那么聚类误差随聚类类别数量增加而变化的幅度应该较不显著，并且也找不到一个合适的K对应数据的真实簇数。<br>
另外，也可以应用霍普金斯统计量（Hopkins Statistic）来判断数据在空间上的随机性。首先，从所有样本中随机找n个点，记为$p_1,p_2,...,p_n$，
对其中的每一个点$p_i$，都在样本空间中找到一个离它最近的点并计算它们之间的距离$x_i$，从而得到距离向量$x_1,x_2,...,x_n$；
然后，从样本的可能取值范围内随机生成n个点，记为$q_1,q_2,...,q_n$，对每个随机生成的点，找到一个离它最近的样本点并计算它们之间的距离，
得到$y_1,y_2,...,y_n$。霍普金斯统计量H可以表示为：
$$ H = \frac{\sum_{i=1}^n y_{i}}{\sum_{i=1}^n x_i + \sum_{i=1}^n y_i} $$
如果样本解决随机分布，那么$\sum_{i=1}^n x_i$和$\sum_{i=1}^n y_i$的取值应该比较接近，即$H$的取值接近于0.5；
如果聚类趋势明显，则随机生成的样本点距离应该远大于实际样本点的距离，即$\sum_{i=1}^n x_i >> \sum_{i=1}^n y_i $，$H$的值接近于1。

2. 判定数据簇数 <br>
确定聚类趋势之后，需要找到与真实数据分布最为吻合的簇数，据此判定聚类结果的质量。数据簇数的判定方法有很多，
例如手肘法和Gap Statistic方法。需要说明的是，用于评估的最佳数据簇数可能与程序输出的簇数是不同的。例如，
有些聚类算法可以自动地确定数据的簇数，但可能与我们通过其他方法确定的最优数据簇数有所差别。

3. 测定聚类质量 <br>
给定预设的簇数，不同的聚类算法将输出不同的结果，如何判定哪个聚类结果的质量更高呢？在无监督的情况下，
可以通过考察簇的分离情况和簇的紧凑情况来评估聚类的效果。定义评估指标可以展现实际解决和分析问题的能力。
事实上测量指标可以有很多种，以下列出了几种常用的度量指标，更多的指标可以阅读相关文献。
    - 轮廓系数 <br>
    给定一个点$p$，该点的轮廓系数定义为：
    $$ s(p) = \frac{b(p) - a(p)}{\max \lbrace a(p), b(p) \rbrace} $$
    其中$a(p)$是点$p$与同一簇中的其他点$p'$之间的平均距离；$b(p)$是点$p$与另一个不同簇中的点之间的最小平均距离（如果有$n$个其他簇，
    则只计算和点$p$最接近的一簇中的点与该点的平均距离）。$a(p)$反应的是$p$所属簇中数据的紧凑程度，$b(p)$反应的是该簇与其他临近簇的分离程度。
    显然，$b(p)$越大，$a(p)$越小，对应的聚类质量越好，因此将所有点对应的轮廓系数$s(p)$求平均值来度量距离结果的质量。
    
    - 均方根标准偏差(Root-mean-square standard deviation，RMSSTD) <br>
    用来衡量聚结果的同质性，即紧凑程度，定义为：
    $$ RMSSTD = \lbrace frac{\sum_i \sum_{x \in C_i} || x - c_i ||^2}{P \sum_i (n_i - 1)} \rbrace^{\frac{1}{2}} $$
    其中$C_i$代表第$i$个簇，$c_i$是该簇的中心，$x \in C_i$代表属于第$i$个簇的一个样本点，$n_i$为第$i$个簇的样本数量，
    $P$为样本点对应的向量维数。可以看出，分母对点的维度$P$做了惩罚，维度越高，则整体的平方距离度量值越大。
    $ \sum_i (n_i - 1) = n - NC $，其中$n$为样本点的总数，$NC$为聚类簇的个数，通常$NC << n$，
    因此$\sum_i (n_i -1) $的值接近点的总数，为一个常数。综上，RMSSTD可以看作是经过归一化的标准差。
    
    - R方(R-Square) <br>
    可以用来衡量聚类的差异度，定义为：
    $$ RS = \frac{ \sum_{x \in D} ||x-c||^2 - \sum_i \sum_{x \in C_i} ||x-c_i||^2 }{ \sum_{x \in D} ||x-c||^2 } $$
    其中$D$代表整个数据集，$c$代表数据集$D$的中心点，从而$\sum_{x \in D} ||x-c||^2$代表将数据集$D$看作单一簇时的平方误差和。
    与上一指标RMSSTD中的定义相同，$\sum_i \sum_{x \in C_i} ||x-c_i||^2 $代表将数据集聚类之后的平方误差和，
    所以$RS$代表了聚类之后的结果与聚类之前相比，对应的平方误差和指标的改进幅度。
    
    - 改进的Hubert$\Gamma$统计 <br>
    通过数据对的不一致性来评估聚类的差异，定义为：
    $$ \Gamma = \frac{2}{n (n-1)} \sum_{x \in D} \sum_{y \in D} d(x,y) d_{x \in C_i, y \in C_j} (C_i, C_j) $$
    其中$d(x,y)$表示点$x$到点$y$之间的距离，$d_{x \in C_i, y \in C_j}$代表点$x$所在的簇中心$c_i$与点$y$所在的簇中心$c_j$之间的距离，
    $\frac{n(n-1)}{2}$为所有$(x,y)$点对的个数，因此指标相当于对每个点对的和做了归一化处理。理想情况下，对于每个点对$(x,y)$，
    如果$d(x,y)$越小，对应的$d_{x \in C_i, y \in C_j}$也应该越小(特别地，当它们属于同一个聚类簇时，$d_{x \in C_i, y \in C_j}=0$)；
    当$d(x,y)$越大时，$d_{x \in C_i, y \in C_j}$的取值也应当越大，所以$\Gamma$值越大说明聚类的结果与样本的原始距离越吻合，也就是聚类质量越高。

此外，为了更加合理地评估不同聚类算法的性能，通常还需要人为地构造不同类型的数据集，以观察聚类算法在这些数据集上的效果，
几个常见的例子如图4所示。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_07.png?raw=true)