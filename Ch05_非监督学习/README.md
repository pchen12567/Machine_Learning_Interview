# 第五章 非监督学习
- 在实际工作中，经常会遇到这样一类问题：给机器输入大量的特征数据，并期望机器通过学习找到数据中存在的某种共性特征或者结构，
亦或是数据之间存在的某种关联。例如，视频网站根据用户的观看行为对用户进行分组从而建立不同的推荐策略，
或是寻找视频播放是否流畅与用户是否退订之间的关系等。这类问题被称作“非监督学习”问题，它并不是像监督学习那样希望预测某种输出结果。
- 相比于监督学习，非监督学习的输入数据没有标签信息，需要通过算法模型来挖掘数据内在的结构和模式。非监督学习主要包含两大类学习方法：
数据聚类和特征变量关联。其中，聚类算法往往是通过多次迭代来找到数据的最优分割，而特征变量关联则是利用各种相关性分析方法来找到变量之间的关系。

## 01 K均值聚类
支持向量机、逻辑回归、决策树等经典的机器学习算法主要用于分类问题，即根据一些已给定类别的样本，训练某种分类器，
使得它能够对类别未知的样本进行分类。与分类问题不同，聚类是在事先并不知道任何样本类别标签的情况下，
通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_01.png?raw=true)

分类问题属于监督学习的范畴，而聚类则是非监督学习。K均值聚类（K-Means Clustering）是最基础和最常用的聚类算法。它的基本思想是，
通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。
特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和：
$$ J(c,\mu) = \sum_{i=1}^M || x_i - \mu_{c_i} ||^2 $$
其中$x_i$代表第$i$个样本，$c_i$是$x_i$所属的簇，$\mu_{c_i}$代表簇对应的中心点，$M$是样本总数。

### 简述K均值算法的具体步骤
K均值聚类的核心目标是将给定的数据集划分成$K$个簇，并给出每个数据对应的簇中心点。算法的具体步骤描述如下：
1. 数据预处理，如归一化、离群点处理等。
2. 随机选取$K$个簇中心，记为$\mu_1^{(0)}, \mu_2^{(0)},..., \mu_K^{(0)}$。
3. 定义损失函数：$ J(c, \mu) = \min_{\mu} \min_{c} \sum_{i=1}^M ||x_i - \mu_{c_i}||^2 $。
4. 令$ t=0,1,2,...$为迭代步数，重复下面的过程直到$J$收敛：
    - 对于每一个样本$x_i$，将其分配到距离最近的簇：
    $$ c_i^{(t)} \leftarrow arg \min_{k} ||x_i - \mu_k^{t}||^2 $$
    - 对于每一个类簇$k$，重新计算该类簇的中心:
    $$ \mu_k^{(t+1)} \leftarrow arg \min_{k} \sum_{i:c_i^{(t)} = k} ||x_i - \mu ||^2 $$
    
K均值算法在迭代时，假设当前$J$没有达到最小值，那么首先固定簇中心$\lbrace \mu_k \rbrace$，调整每个样例$x_i$所属的类别$c_i$来让$J$函数减少；
然后固定$\lbrace c_i \rbrace$，调整簇中心$\lbrace \mu_k \rbrace$使$J$减小。这两个过程交替循环，$J$单调递减：当$J$递减到最小值时，
$\lbrace \mu_k \rbrace$和$\lbrace c_i \rbrace$也同时收敛。

K_means算法的迭代过程如图所示：首先，给定二维空间上的一些样本点，直观上这些点可以被分成两类；
接下来，初始化两个中心点（棕色和黄色叉子代表中心点），并根据中心点的位置计算每个样本所属的簇；
然后根据每个簇中的所有点的平均值计算新的中心点位置；图（e）和图（f）展示了新一轮的迭代结果；在经过两轮的迭代之后，算法基本收敛。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_02.png?raw=true)

### K均值算法的优缺点是什么？如何对其进行调优？
K均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、
无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类等。但是瑕不掩瑜，
K均值聚类的优点也是很明显和突出的，主要体现在：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是$O(NKt)$接近于线性，
其中$N$是数据对象的数目，$K$是聚类的簇数，$t$是迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。

K均值算法的调优一般可以从以下几个角度出发。
1. 数据归一化和离群点处理。<br>
K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，
所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，
因此使用K均值聚类算法之前通常需要对数据做预处理。

2. 合理选择K值。<br>
K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，
或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，可以尝试不同的K值，
并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数，如图所示：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_03.png?raw=true)<br>
由图可见，$K$值越大，距离和越小；并且，当$K=3$时，存在一个拐点，就像人的肘部一样；当$K \in (1,3)$时，曲线急速下降；
当$K>3$时，曲线趋于平稳。手肘法认为拐点就是$K$的最佳值。

    手肘法是一个经验方法，缺点就是不够自动化，因此又提出了一些更先进的方法，其中包括比较有名的Gap Statistic方法。
    Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的Gap statistic所对应的$K$即可，因此该方法也适用于批量化作业。
    在这里继续使用上面的损失函数，当分为$K$簇时，对应的损失函数记为$D_k$。Gap Statistic定义为：
    $$ Gap(K) = E(\log{D_k}) - \log{D_k} $$
    其中$E(\log{D_k})$是$\log{D_k}$的期望，一般通过蒙特卡洛模拟产生。
    在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做$K$均值，得到一个$D_k$；
    重复多次就可以计算出$E(\log{D_k})$的近似值。那么Gap(K)有什么物理含义呢？它可以视为随机样本的损失与实际样本的损失之差。
    试想实际样本对应的最佳簇数为$K$，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，
    从而Gap(K)取得最大值所对应的K值就是最佳的簇数。根据上式计算$K =1,2,...,9$所对应的Gap Statistic，如图所示：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_04.png?raw=true)<br>
    由图可见，当K=3时，Gap(K)取值最大，所以最佳的簇数是K=3。

3. 采用核函数。<br>
采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，
并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，
这时算法又称为核K均值算法，是核聚类方法的一种。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，
并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。

### 针对K均值算法的缺点，有哪些改进的模型？
- K均值算法的主要缺点如下。
    1. 需要人工预先确定初始K值，且该值和真实的数据分布未必吻合。
    2. K均值只能收敛到局部最优，效果受到初始值很大。
    3. 易受到噪点的影响。
    4. 样本点只能被划分到单一的类中。
    
- K-means++ 算法
K均值的改进算法中，对初始值选择的改进是很重要的一部分。而这类算法中，最具影响力的当属K-means++算法。
原始K均值算法最开始随机选取数据集中$K$个点作为聚类中心，而K-means++按照如下的思想选取K个聚类中心。
假设已经选取了$n$个初始聚类中心$(0<n<K)$，则在选取第$n+1$个聚类中心时，距离当前$n$个聚类中心越远的点会有更高的概率被选为第$n+1$个聚类中心。
在选取第一个聚类中心$(n=1)$时同样通过随机的方法。可以说这也符合我们的直觉，聚类中心当然是互相离得越远越好。
当选择完初始点后，K-means++后续的执行和经典K均值算法相同，这也是对初始值选择进行改进的方法等共同点。

- ISODATA算法
当K值的大小不确定时，可以使用ISODATA算法。ISODATA的全称是迭代自组织数据分析法。在K均值算法中，聚类个数K的值需要预先人为地确定，
并且在整个算法过程中无法更改。而当遇到高维度、海量的数据集时，人们往往很难准确地估计出K的大小。ISODATA算法就是针对这个问题进行了改进，
它的思想也很直观。当属于某个类别的样本数过少时，把该类别去除；当属于某个类别的样本数过多、分散程度较大时，把该类别分为两个子类别。
ISODATA算法在K均值算法的基础之上增加了两个操作，一是分裂操作，对应着增加聚类中心数；二是合并操作，对应着减少聚类中心数。
ISODATA算法是一个比较常见的算法，其缺点是需要指定的参数比较多，不仅仅需要一个参考的聚类数量$K_o$，还需要制定3个阈值。
下面介绍ISODATA算法的各个输入参数。
    1. 预期的聚类中心数目$K_o$。在ISODATA运行过程中聚类中心数可以变化，$K_o$是一个用户指定的参考值，
    该算法的聚类中心数目变动范围也由其决定。具体地，最终输出的聚类中心数目常见范围是从$K_o$的一半，到两倍$K_o$。
    2. 每个类所要求的最少样本数目$N_{\min}$。如果分裂后会导致某个子类别所包含样本数目小于该阈值，就不会对该类别进行分裂操作。
    3. 最大方差Sigma。用于控制某个类别中样本的分散程度。当样本的分散程度超过这个阈值时，且分裂后满足（1），进行分裂操作。
    4. 两个聚类中心之间所允许最小距离$D_{\min}$。如果两个类靠得非常近（即这两个类别对应聚类中心之间的距离非常小），小于该阈值时，
    则对这两个类进行合并操作。

    如果希望样本不划分到单一的类中，可以使用模糊C均值或者高斯混合模型。
    
### 证明K均值算法的收敛性
首先，需要知道K均值聚类的迭代算法实际上是一种最大期望算法（Expectation-Maximization algorithm），简称EM算法。
EM算法解决的是在概率模型中含有无法观测的隐含变量情况下的参数估计问题。假设有m个观察样本，模型的参数为θ，
最大化对数似然函数可以写成如下形式：
$$ \theta = arg \max_{\theta} \sum_{i=1}^m \log{P(x^{ (i)}| \theta )} $$