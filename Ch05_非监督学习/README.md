# 第五章 非监督学习
- 在实际工作中，经常会遇到这样一类问题：给机器输入大量的特征数据，并期望机器通过学习找到数据中存在的某种共性特征或者结构，
亦或是数据之间存在的某种关联。例如，视频网站根据用户的观看行为对用户进行分组从而建立不同的推荐策略，
或是寻找视频播放是否流畅与用户是否退订之间的关系等。这类问题被称作“非监督学习”问题，它并不是像监督学习那样希望预测某种输出结果。
- 相比于监督学习，非监督学习的输入数据没有标签信息，需要通过算法模型来挖掘数据内在的结构和模式。非监督学习主要包含两大类学习方法：
数据聚类和特征变量关联。其中，聚类算法往往是通过多次迭代来找到数据的最优分割，而特征变量关联则是利用各种相关性分析方法来找到变量之间的关系。

## 01 K均值聚类
支持向量机、逻辑回归、决策树等经典的机器学习算法主要用于分类问题，即根据一些已给定类别的样本，训练某种分类器，
使得它能够对类别未知的样本进行分类。与分类问题不同，聚类是在事先并不知道任何样本类别标签的情况下，
通过数据之间的内在关系把样本划分为若干类别，使得同类别样本之间的相似度高，不同类别之间的样本相似度低。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_01.png?raw=true)

分类问题属于监督学习的范畴，而聚类则是非监督学习。K均值聚类（K-Means Clustering）是最基础和最常用的聚类算法。它的基本思想是，
通过迭代方式寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的代价函数最小。
特别地，代价函数可以定义为各个样本距离所属簇中心点的误差平方和：
$$ J(c,\mu) = \sum_{i=1}^M || x_i - \mu_{c_i} ||^2 $$
其中$x_i$代表第$i$个样本，$c_i$是$x_i$所属的簇，$\mu_{c_i}$代表簇对应的中心点，$M$是样本总数。

### 简述K均值算法的具体步骤
K均值聚类的核心目标是将给定的数据集划分成$K$个簇，并给出每个数据对应的簇中心点。算法的具体步骤描述如下：
1. 数据预处理，如归一化、离群点处理等。
2. 随机选取$K$个簇中心，记为$\mu_1^{(0)}, \mu_2^{(0)},..., \mu_K^{(0)}$。
3. 定义损失函数：$ J(c, \mu) = \min_{\mu} \min_{c} \sum_{i=1}^M ||x_i - \mu_{c_i}||^2 $。
4. 令$ t=0,1,2,...$为迭代步数，重复下面的过程直到$J$收敛：
    - 对于每一个样本$x_i$，将其分配到距离最近的簇：
    $$ c_i^{(t)} \leftarrow arg \min_{k} ||x_i - \mu_k^{t}||^2 $$
    - 对于每一个类簇$k$，重新计算该类簇的中心:
    $$ \mu_k^{(t+1)} \leftarrow arg \min_{k} \sum_{i:c_i^{(t)} = k} ||x_i - \mu ||^2 $$
    
K均值算法在迭代时，假设当前$J$没有达到最小值，那么首先固定簇中心$\lbrace \mu_k \rbrace$，调整每个样例$x_i$所属的类别$c_i$来让$J$函数减少；
然后固定$\lbrace c_i \rbrace}$，调整簇中心$\lbrace \mu_k \rbrace$使$J$减小。这两个过程交替循环，$J$单调递减：当$J$递减到最小值时，
$\lbrace \mu_k \rbrace$和$\lbrace c_i \rbrace}$也同时收敛。

K_means算法的迭代过程如图所示：首先，给定二维空间上的一些样本点，直观上这些点可以被分成两类；
接下来，初始化两个中心点（棕色和黄色叉子代表中心点），并根据中心点的位置计算每个样本所属的簇；
然后根据每个簇中的所有点的平均值计算新的中心点位置；图（e）和图（f）展示了新一轮的迭代结果；在经过两轮的迭代之后，算法基本收敛。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_02.png?raw=true)

### K均值算法的优缺点是什么？如何对其进行调优？
K均值算法有一些缺点，例如受初值和离群点的影响每次的结果不稳定、结果通常不是全局最优而是局部最优解、
无法很好地解决数据簇分布差别比较大的情况（比如一类是另一类样本数量的100倍）、不太适用于离散分类等。但是瑕不掩瑜，
K均值聚类的优点也是很明显和突出的，主要体现在：对于大数据集，K均值聚类算法相对是可伸缩和高效的，它的计算复杂度是$O(NKt)$接近于线性，
其中$N$是数据对象的数目，$K$是聚类的簇数，$t$是迭代的轮数。尽管算法经常以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。

K均值算法的调优一般可以从以下几个角度出发。
1. 数据归一化和离群点处理。<br>
K均值聚类本质上是一种基于欧式距离度量的数据划分方法，均值和方差大的维度将对数据的聚类结果产生决定性的影响，
所以未做归一化处理和统一单位的数据是无法直接参与运算和比较的。同时，离群点或者少量的噪声数据就会对均值产生较大的影响，导致中心偏移，
因此使用K均值聚类算法之前通常需要对数据做预处理。

2. 合理选择K值。<br>
K值的选择是K均值聚类最大的问题之一，这也是K均值聚类算法的主要缺点。实际上，我们希望能够找到一些可行的办法来弥补这一缺点，
或者说找到K值的合理估计方法。但是，K值的选择一般基于经验和多次实验结果。例如采用手肘法，可以尝试不同的K值，
并将不同K值所对应的损失函数画成折线，横轴为K的取值，纵轴为误差平方和所定义的损失函数，如图所示：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_03.png?raw=true)<br>
由图可见，$K$值越大，距离和越小；并且，当$K=3$时，存在一个拐点，就像人的肘部一样；当$K \in (1,3)$时，曲线急速下降；
当$K>3$时，曲线趋于平稳。手肘法认为拐点就是$K$的最佳值。

    手肘法是一个经验方法，缺点就是不够自动化，因此又提出了一些更先进的方法，其中包括比较有名的Gap Statistic方法。
    Gap Statistic方法的优点是，不再需要肉眼判断，而只需要找到最大的Gap statistic所对应的$K$即可，因此该方法也适用于批量化作业。
    在这里继续使用上面的损失函数，当分为$K$簇时，对应的损失函数记为$D_k$。Gap Statistic定义为：
    $$ Gap(K) = E(\log{D_k}) - \log{D_k} $$
    其中$E(\log{D_k})$是$\log{D_k}$的期望，一般通过蒙特卡洛模拟产生。
    在样本所在的区域内按照均匀分布随机地产生和原始样本数一样多的随机样本，并对这个随机样本做$K$均值，得到一个$D_k$；
    重复多次就可以计算出$E(\log{D_k})$的近似值。那么Gap(K)有什么物理含义呢？它可以视为随机样本的损失与实际样本的损失之差。
    试想实际样本对应的最佳簇数为$K$，那么实际样本的损失应该相对较小，随机样本损失与实际样本损失之差也相应地达到最小值，
    从而Gap(K)取得最大值所对应的K值就是最佳的簇数。根据上式计算$K =1,2,...,9$所对应的Gap Statistic，如图所示：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/k_means_04.png?raw=true)<br>
    由图可见，当K=3时，Gap(K)取值最大，所以最佳的簇数是K=3。

3. 采用核函数。<br>
采用核函数是另一种可以尝试的改进方向。传统的欧式距离度量方式，使得K均值算法本质上假设了各个数据簇的数据具有一样的先验概率，
并呈现球形或者高维球形分布，这种分布在实际生活中并不常见。面对非凸的数据分布形状时，可能需要引入核函数来优化，
这时算法又称为核K均值算法，是核聚类方法的一种。核聚类方法的主要思想是通过一个非线性映射，将输入空间中的数据点映射到高位的特征空间中，
并在新的特征空间中进行聚类。非线性映射增加了数据点线性可分的概率，从而在经典的聚类算法失效的情况下，通过引入核函数可以达到更为准确的聚类结果。
