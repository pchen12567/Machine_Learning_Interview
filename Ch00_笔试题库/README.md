
## 01 百度笔试模拟题
1. 符号集a、b、c、d,它们相互独立,相应概率为1/2、1/4、1/8/、1/16,其中包含信息量最小的符号是( )

    a
    ```
    因为消息出现的概率越小，则消息中所包含的信息量就越大。
    ```

2. 要求通过函数来实现一种不太复杂的功能,并且要求加快执行速度,选用( )。
    - 递归调用
    - 嵌套调用
    - 内联函数
    - 重载函数
    
    内联函数

3. 最坏情况下时间复杂度不是n(n-1)/2的排序算法是?( )
    - 直接插入排序
    - 快速排序
    - 冒泡排序
    - 堆排序
    
    堆排序

4. 以下哪种方法不属于特征选择的标准方法( )。
    - 包装
    - 过滤
    - 嵌入
    - 抽样
    
    抽样

5. 在机器学习的实验过程中常常使用K折交叉验证（k-fold cross validation）方法，下面关于该方法的描述正确的有( )
    - 该方法的最终结果是取K次实验的最好结果 
    - 该方法中K的取值一般是大于等于1 
    - 该方法可以避免训练集和测试集数据分配偶然性对实验结果的影响
    
    ？（不定项）
    
6. 假定你使用了一个很大γ值的RBF核,这意味着( )。
    - 模型将考虑使用远离超平面的点建模
    - 模型仅使用接近超平面的点来建模
    - 模型不会被点到超平面的距离所影响
    - 以上都不正确
    
    模型仅使用接近超平面的点来建模
    
7. 假如我们使用非线性可分的SVM目标函数作为最优化对象, 我们怎么保证模型线性可分?( )
    - 设C=1
    - 设C=0
    - 设C=无穷大
    - 以上都不对
    
    设C=无穷大
    ```
    C=无穷大保证了所有的线性不可分都是可以忍受的.
    ```
    
8. 在Python中，表达式list1=list(map(lambda x:x*2,[2,2,3,4,'ok']))执行之后，list1[-1]返回值是( )
    
    ```
    'okok'
    ```
    
9. 在数据库中，第二范式是在第一范式的基础上消除了非主属性对____的部分函数依赖。
    
    键
    ```
    范式是符合某一种级别的关系模式的集合。关系数据库中的关系必须满足一定的要求，满足不同程度要求的为不同范式。
    目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd范式（BCNF）、第四范式（4NF）
    和第五范式（5NF）。满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多要求的称为第二范式（2NF），
    其余范式以次类推。一般说来，数据库只需满足第三范式（3NF）就行了。第一范式：主属性（主键）不为空且不重复，字段不可再分
    （存在非主属性对主属性的部分依赖）第二范式（2NF）：首先是 1NF，另外包含两部分内容，一是表必须有一个主键；
    二是没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分.第三范式：如果关系模式是第二范式，
    没有非主属性对主键的传递依赖和部分依赖。BCNF范式：所有属性都不传递依赖于关系的任何候选键。
    ```
    
10. 字符串acomnad与command之间的编辑距离（Levenshtein距离）是___

    4？
    
11. 假定你已经搜集了10000行推特文本的数据,不过没有任何信息。现在你想要创建一个推特分类模型,好把每条推特分为三类:积极、消极、中性。
以下哪个模型可以执行做到?( )
    - 朴素贝叶斯
    - 支持向量机
    - 以上都不是
    
    以上都不是
    ```
    由于被给定了推特数据并且没有其他信息，这意味着不存在目标变量，所以不可能训练一个监督学习模型，
    支持向量机和朴素贝叶斯都是监督学习技巧。
    ```

12. 分词是自然语言之中文处理的一个先决步骤,分词的主要障碍是歧义。最简单的分词算法是正向最大分词和逆向最大分词,下面说法正确的是( )。
    - 正向最大分词的切分精度高于逆向最大分词
    - 正向最大分词的切分精度等于逆向最大分词
    - 正向最大分词的切分精度低于逆向最大分词
    - 正向最大分词的逆向最大分词的切分精度不可以比较
    
    正向最大分词的切分精度低于逆向最大分词
    ```
    英文的分词由于单词间是以空格进行分隔的，所以分词要相对的容易些，而中文就不同了，中文中一个句子的分隔就是以字为单位的了，
    而所谓的正向最大匹配和逆向最大匹配便是一种分词匹配的方法，这里以词典匹配说明。

    所谓词典正向最大匹配就是将一段字符串进行分隔，其中分隔的长度有限制，然后将分隔的子字符串与字典中的词进行匹配，
    如果匹配成功则进行下一轮匹配，直到所有字符串处理完毕，否则将子字符串从末尾去除一个字，再进行匹配，如此反复。
    逆向匹配与此类似。
    
    正向匹配：从左到右，逐步去掉右部（底部）的字进行新一轮匹配，逆向匹配：从右到左，逐步去掉左部（底部）的字进行新一轮匹配。
    
    因为中文比较复杂以及中文的特殊性，逆向最大匹配大多时候往往会比正向要准确。
    ```
    
13. Python类中的变量一般不允许直接修改,否则会破坏面向对象( )。
    - 封装特性
    - 继承特性
    - 多态特性
    - 以上都对
    
    封装特性
    
14. 邻接矩阵一定为对称矩阵的图是?( )
    - 有向图或无向图
    - 无向图
    - 有向图
    - 带权有向图
    
    无向图
    ```
    无向图的邻接矩阵一定是对称的，因为任意两个结点之间如果有连接，则连接是对等的。
    
    有向图的邻接矩阵有可能是对称矩阵，假设任意两个结点之间如果有连接就是双向连接，这种情况下邻接矩阵就是对称矩阵。
    ```

15. 只有非零值才重要的二元属性被称作( )。
    - 对称属性
    - 非对称的二元属性
    - 计数属性
    - 离散属性
    
    非对称的二元属性
    
16. logistic回归与多元回归分析有哪些不同?( )
    - logistic回归预测某事件发生的概率
    - logistic回归有较高的拟合效果
    - logistic回归回归系数的评估
    - 以上全选
    
    以上全选
    
17. 关于Ridge和Lasso回归在特征值选择上的方法,以下哪项正确?( )
    - Ridge回归使用特征值的子集选择
    - Lasso回归使用特征值的子集选择
    - 二者都使用特征值的子集选择
    - 以上都不正确
    
    Lasso回归使用特征值的子集选择
    ```
    Ridge回归在最终模型中用到了所有自变量，然而Lasso回归可被用于特征值选择，因为相关系数可以为零。
    ```

18. 建立一个贝叶斯模型需要__个概率值。
    
    2?
    
19. 平面上有三条平行直线，每条直线上分别有7，5，6个点，且不同直线上三个点都不在同一条直线上。
问用这些点为顶点，能组成多少个不同三角形？    

    ```
    C(7,2)*(5+6)+C(5,2)*(7+6)+C(6,2)*(7+5)+7*6*5=21*11+10*13+15*12+210=231+130+180+210=751
    
    四边形为：21*10+21*15+10*15+21*30+10*42+15*35=1155+525+570=2250
    ```
    
20. 利用归并排序方法对数字序列:5,19,17,21,11,8,1进行排序，共需要进行（）次比较。
    
    11
    ```
    首先第一趟归并，以一个元素为子表长
    5__19   17__21   11__8   1  分为四组，前三组进行两两比较，共比较3次
    一趟归并之后：5__19   17__21   8__11  1
    
    第二次归并子表长为2，前两个子表和后两个子表分别进行归并：
    5__19__17__21   8__11__1
    前两个子表归并：5和17进行比较，17和19进行比较，19和21进行比较，共3次
    后两个子表归并：8和1比较,1放入有序序列，8__11已经有序，不需要再比较，共1次
    二趟归并共比较了3+1=4次
    二趟归并之后：5__17__19__21   1__8__11
    
    第三次归并子表长为4，将两个子表进行归并：
    5和1比较，1进入有序序列，5和8比较，5进入有序序列，然后是17和8比较，8进入有序序列，17和11比较，11进入有序序列，
    剩下的17到21已经有序。共比较了4次，归并后：1__5__8__11__17__19__21
    
    共进行了3趟归并，共比较了3+4+4=11次
    ```
      
21. Word2vec模型是一种用于给文本目标创建矢量标记的机器学习模型。对于Word2vec,它包含多个深度神经网络。这种说法是否正确?( )
    - 正确
    - 错误
    
    错误
    ```
    Word2vec 也包含预处理模型（preprocessing mode），它不属于深度神经网络。
    ```

22. 北京到广州的距离为a,有一辆火车以每小时15公里的速度离开北京直奔广州,同时另一辆火车每小时20 公里的速度从广州开往北京。
如果有一只鸟,以30公里每小时的速度和两辆火车同时启动,从北京出发,碰到另一辆车后就向相反的方向返回去飞,
就这样依次在两辆火车之间来回地飞,直到两辆火车相遇。请问,这只鸟约飞行了多少公里?( )
    - (4/7)*a
    - (3/7)*a
    - (1/7)*a
    - (6/7)*a
    
    (30/(15+20))*a = (6/7)*a
    
23. 邻接矩阵一定为对称矩阵的图是?( )
    - 有向图或无向图
    - 无向图
    - 有向图
    - 带权有向图
    
24. 在图集合中发现一组公共子结构,这样的任务称为( )。
    - 频繁子集挖掘
    - 频繁数据项挖掘
    - 频繁模式挖掘
    - 频繁子图挖掘
    
    频繁子图挖掘
    
25. 分析逻辑回归表现的一个良好的方法是AIC,它与线性回归中的R平方相似。有关AIC,以下哪项是正确的?( )
    - 具有最小AIC值的模型更好
    - 具有最大AIC值的模型更好
    - 视情况而定
    - 以上都不是
    
    具有最小AIC值的模型更好
    ```
    AIC信息准则即Akaike information criterion，是衡量统计模型拟合优良性的一种标准，
    由于它为日本统计学家赤池弘次创立和发展的，因此又称赤池信息量准则。
    考虑到AIC=2k-2In(L) ，所以一般而言，当模型复杂度提高（k增大）时，似然函数L也会增大，从而使AIC变小，
    但是k过大时，似然函数增速减缓，导致AIC增大，模型过于复杂容易造成过拟合现象。
    目标是选取AIC最小的模型，AIC不仅要提高模型拟合度（极大似然），而且引入了惩罚项，使模型参数尽可能少，有助于降低过拟合的可能性。
    综上，我们一般选择逻辑回归中最少的AIC作为最佳模型。
    ```

26. 已知一组数据的协方差矩阵P,下面关于主分量说法错误的是( )。
    - 主分量分析的最佳准则是对一组数据进行按一组正交基分解, 在只取相同数量分量的条件下,以均方误差计算截尾误差最小
    - 在经主分量分解后,协方差矩阵成为对角矩阵
    - 主分量分析就是K-L变换
    - 主分量是通过求协方差矩阵的特征值得到
    
    主分量分析就是K-L变换
    ```
    K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，
    K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。
    当K-L变换矩阵为协方差矩阵时，等同于PCA。
    ``` 
    
27. 以下描述错误的是( )。
    - 最长路径问题有多项式时间解
    - PageRank算法总是会收敛
    - KMP算法的时间复杂度是O(n)
    - 最大流问题和最小割问题是等价的
    
    最长路径问题有多项式时间解-错误
    ```
    KMP时间复杂度是O(m+n)，m,n是主串，子串长度。
    最长路径问题是NP问题，还没证实存在多项式时间复杂度。
    大名鼎鼎的PageRank算法，由google扛把子发明。数学上可以证明是收敛的。
    ``` 
    
28. 在有n个结点的二叉链表中,值为非空的链域的个数为___。
    
    n-1
    ```
    在有N个结点的二叉链表中必定有2N个链域。
    除根结点外，其余N-1个结点都有一个父结点。
    所以，一共有N-1个非空链域，其余 2N -（N-1）= N+1个为空链域。
    ```
    
29. 在Python中，通过类内部的装饰器___,可以指定类的方法变为属性。

    @property
    
30. 对于句子 There is no royal road to learning , 使用bigram模型，
计算经过Add-k Smoothing（Lidstone's law）之后的概率p(to|road)是__________，
其中k=0.02。（写成最简分数形式，如：A/B）

    51/400 ?

31. 下列有关串，说法正确的是（）
    - 空串与空格相等
    - 空格串的串长度为0
    - 除主串S本身外，S的其他子串称为S的真子串
    - 子串在主串的位置是子串最后一个字符在主串的位置
    
    除主串S本身外，S的其他子串称为S的真子串

32. 利用折半查找算法要在有序表(17,21,34,38,47,54,70)查找元素34需经过（）次比较
    - 2
    - 3
    - 5
    - 4
    
    3
    ```
    算法思想：
    首先，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；
    否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，
    否则进一步查找后一子表。
    重复以上步骤，直到找到满足条件的结果为止，若找不到，则返回失败。
    
    38-->21-->34
    ```

33. 已知序列(50,30,80,20,40,90,35,85,32,88)，按照依次插入的方法生成二叉排序树，则在该树中删除关键字值为50的节点后，
其二叉排序树的根节点的值可能为()
    - 30
    - 40
    - 90
    - 80
    
    80
    ```
    当删除根节点时，应将根节点指向其右子树的最左，即最小的节点
    ```

34. 序列[9,14,11,16,21,15,20,31]为小顶堆，在删除堆顶元素9之后，调整后的结果是（）
    - [14,11,16,21,15,20,31]
    - [11,14,16,21,15,20,31]
    - [11,14,15,16,21,31,20]
    - [11,14,15,16,20,21,31]

    [11,14,15,16,21,31,20]
    ```
    直接建立初始最大堆，然后交换堆顶和最后节点的值，然后进行堆的调整！
    ```

35. 对任意数列进行排序时，平均排序时间最短的排序算法是()
    - 插入排序
    - 归并排序
    - 快速排序
    - 堆排序
    
    归并排序
    
36. 下列说法错误的是（）
    - 聚类分析可以看作是一种非监督的分类
    - 在聚类分析中，簇内的相似性越大，簇间的差别越大，聚类的效果就越差
    - 给定由两次运行K均值产生的两个不同的簇集，误差的平方和最大的那个应该被视为较优
    - K均值是一种产生划分聚类的基于密度的聚类算法，簇的个数有算法自动的确定

    除了第一个，其他三个都错
    
37. 有一家医院为了研究癌症的诊断，对一大批人作了一次普查，给每人打了试验针，然后进行统计，得到如下统计数字：<br>
(一) 这批人中，每1000人有5个癌症病人<br>
(二) 这批人中，每100个正常人有1人对试验的反应为阳性<br>
(三) 这批人中，每100个癌症病人有95人对试验的反应为阳性<br>
通过普查统计，该医院可开展癌症诊断。
现在某人试验结果为阳性，根据最小风险贝叶斯决策理论，将此患者预测为患癌症的风险概率为（）<br>
假设将正常人预测为正常人和将癌症患者预测为癌症患者的损失函数均为0，
将癌症患者预测为正常人的损失函数为3，将正常人预测为癌症患者的损失函数为1.
    - 75.50%
    - 96.9%
    - 67.7%
    - 32.3%
    
    67.7%
    
38. 关于数组，以下说法正确的是()
    - 数组也是一种线性表
    - 部分数组时线性表
    - 数组不是线性表
    - 只有一维数组时线性表  
    
    数组也是一种线性表 ？  

39. TCP协议规定DNS进程的（）端口号是53
    - 服务器
    - 主机
    - 客户
    - 分布
    
    服务器   

40. 给定文法G[E]:E->E+E|E*E|id(id表示任意小写字母)，下列表达式合法的是（）
    - a+b*c
    - a-(b+c)
    - a*(b+c)
    - a-b-c
    
    a+b*c ？    

41. 在分时系统中，时间片设置等于3，以下关于相应时间的分析，正确的是（）
    - 用户数量越多响应时间越长
    - 进程数量越多响应时间越长
    - 时间片越小响应时间越长
    - 内存空间越大响应时间越长 
    
    用户数量越多响应时间越长，进程数量越多响应时间越长 ？   
    
42. 信息检索系统的评价指标有哪些（）
    - 响应时间
    - 精确度
    - 空间消耗
    - 召回率    
    
    以上全部
    ```
    信息检索的目的是：较少消耗情况下尽快尽全面地返回正确的结果(包含了以上四个选项)
    ```

43. 设一个栈的输入序列为1，2，3，4，5，6，则借助一个栈得到的输出序列不可能是（）
    - 3，1，5，4，2，6
    - 2，4，3，1，6，5
    - 1，2，3，4，5，6
    - 2，3，1，5，6，4
    
    3，1，5，4，2，6 不可能
    ```
    栈：先进后出
    ```   
     
44. 下列关于现有的分词算法说法错误的是（）
    - 中文分词的准确度，对搜索引擎结果相关性和准确性有相当大的关系
    - 由于在上下文中，相邻的字同时出现的次数越多，就越有可能构成一个词，统计词料中频度可以判断是否构成一个词
    - 统计分词系统将串频统计和串匹配结合起来，既发挥匹配分词切分速度快，效率高的特点，又利用了无词典分词结合上下文识别生词，自动消除歧义的优点
    - 基于统计的分词方法是总控部分的协调下，分词子系统获得有关词，句子等的句法和语义信息来对分词歧义进行判断  
    
    基于统计的分词方法是总控部分的协调下，分词子系统获得有关词，句子等的句法和语义信息来对分词歧义进行判断  错误 ？ 
     
45. 下列关于数据降维方法正确的是()
    - 核化主成分分析为先将样本映射到高维空间，再在高维空间中使用线性降维
    - PCA采用一组新的基来表示样本点，每个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而实现降维
    - 流行学习是一种借助拓扑流形概念的降维方法，采用的思想是“邻域保持”
    - MDS要求原始样本空间样本之间的距离在降维后的低维空间得以保持  
    
    以上都正确  
    
46. 如当前样本集合D中的K类样本所占的比例为P(k)(k=1,2,3,…,y)，则样本的信息熵最大值是（）
    - 0.5
    - Log2(p(y))
    - Log2(y)
    - 1    
    
    Log2(y)
    
47. 某数据存放在DS=2000H和DI=1234H的数据段的存储单元中，则该存储单元的物理地址是（）
    - 3234H
    - 14340H
    - 21234H
    - 其他几项都不对 
    
    21234H ？   
    
48. 下列属于常用的风箱方法的是（）
    - 统一区间法
    - 平均值法
    - 统一权重法
    - 自定义区间法   
    
    统一区间法 统一权重法 自定义区间法   ？
    
47. 在Linux系统中查看正在通过ssh登陆的用户和终端（）
    - Who | awk ‘{print 1 ,1,3}’
    - Login | awk ‘{print 1,1,3}’
    - Who | awk ‘{print 0 ,0,2}’
    - Ssh  | awk ‘{print 1,1,3}’    

    who | awk '{print $1,$3}'    ?    
    
48. 下列关于语言模型的说法错误的是（）
    - 基于语料库的统计分析需要从大规模的真实语言中发现知识
    - 基于知识的语言模型是经验主义方法
    - 基于语料库的统计模型更加注重数学的方法
    - 基于知识的语言模型通过非歧义的规则解释歧义过程
   
    基于知识的语言模型是经验主义方法 --> 错误，是非经验主义的方法        
    
## 02 机器学习笔试面试题
> https://blog.csdn.net/abc_138/article/details/82798674

1. 下列时间序列模型中,哪一个模型可以较好地拟合波动性的分析和预测。
    - AR模型
    - MA模型
    - ARMA模型
    - GARCH模型
    
2. 以下说法中错误的是（）
    - SVM对噪声（如来自其他分部的噪声样本）具备鲁棒性
    - 在adaboost算法中，所有被分错样本的权重更新比例不相同
    - boosting和bagging都是组合多个分类器投票的方法，二者都是根据单个分类器的正确率确定其权重
    - 给定n个数据点，如果其中一半用于训练，一半用户测试，则训练误差和测试误差之间的差别会随着n的增加而减少的

3. 你正在使用带有 L1 正则化的 logistic 回归做二分类，其中 C 是正则化参数，w1 和 w2 是 x1 和 x2 的系数。
当你把 C 值从 0 增加至非常大的值时，下面哪个选项是正确的？
    - 第一个 w2 成了 0，接着 w1 也成了 0
    - 第一个 w1 成了 0，接着 w2 也成了 0
    - w1 和 w2 同时成了 0
    - 即使在 C 成为大值之后，w1 和 w2 都不能成 0
    
4. 在 k-均值算法中，以下哪个选项可用于获得全局最小？    
    - 尝试为不同的质心（centroid）初始化运行算法
    - 调整迭代的次数
    - 找到集群的最佳数量
    - 以上所有    
    
5. 假设你使用 log-loss 函数作为评估标准。下面这些选项，哪些是对作为评估标准的 log-loss 的正确解释。
    - 如果一个分类器对不正确的分类很自信，log-loss 会严重的批评它
    - 对一个特别的观察而言，分类器为正确的类别分配非常小的概率，然后对 log-loss 的相应分布会非常大
    - log-loss 越低，模型越好
    - 以上都是    
    
6. 下面哪个选项中哪一项属于确定性算法？    
    - PCA
    - K-Means
    - 以上都不是    
    
7. 两个变量的 Pearson 相关性系数为零，但这两个变量的值同样可以相关。这句描述是正确还是错误？
    - 正确
    - 错误    
    
8. 下面哪个/些超参数的增加可能会造成随机森林数据过拟合？
    - 树的数量
    - 树的深度
    - 学习速率    
    
9. 下列哪个不属于常用的文本分类的特征选择算法？
    - 卡方检验值
    - 互信息
    - 信息增益
    - 主成分分析  
    
10. 机器学习中做特征选择时，可能用到的方法有？
    - 卡方
    - 信息增益
    - 平均互信息
    - 期望交叉熵
    - 以上都有    

11. 下列方法中，不可以用于特征降维的方法包括    
    - 主成分分析PCA
    - 线性判别分析LDA
    - 深度学习SparseAutoEncoder
    - 矩阵奇异值分解SVD
        
12. 下列哪些不特别适合用来对高维数据进行降维    
    - LASSO
    - 主成分分析法
    - 聚类分析
    - 小波分析法
    - 线性判别法
    - 拉普拉斯特征映射     
    
13. 下列属于无监督学习的是
    - k-means
    - SVM
    - 最大熵
    - CRF   
     
14. 下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）    
    - 特征灵活
    - 速度快
    - 可容纳较多上下文信息
    - 全局最优    

15. 以下哪个是常见的时间序列算法模型
    - RSI
    - MACD
    - ARMA
    - KDJ    

16. 下列不是SVM核函数的是
    - 多项式核函数
    - logistic核函数
    - 径向基核函数
    - Sigmoid核函数    
    
17. 解决隐马模型中预测问题的算法是   
    - 前向算法
    - 后向算法
    - Baum-Welch算法
    - 维特比算法 

18. 一般，k-NN最近邻方法在（）的情况下效果较好    
    - 样本较多但典型性不好
    - 样本较少但典型性好
    - 样本呈团状分布
    - 样本呈链状分布    
    
19. 在一个n维的空间中， 最好的检测outlier(离群点)的方法是（）
    - 作正态分布概率图
    - 作盒形图
    - 马氏距离
    - 作散点图    

20. 对数几率回归（logistics regression）和一般回归分析有什么区别？    
    - 对数几率回归是设计用来预测事件可能性的
    - 对数几率回归可以用来度量模型拟合程度
    - 对数几率回归可以用来估计回归系数
    - 以上所有    
    
21. bootstrap数据是什么意思？（提示：考“bootstrap”和“boosting”区别）    
    - 有放回地从总共M个特征中抽样m个特征
    - 无放回地从总共M个特征中抽样m个特征
    - 有放回地从总共N个样本中抽样n个样本
    - 无放回地从总共N个样本中抽样n个样本   

22. “过拟合”只在监督学习中出现，在非监督学习中，没有“过拟合”，这是（）
    - 对的
    - 错的

23. 对于k折交叉验证, 以下对k的说法正确的是（）
    - k越大, 不一定越好, 选择大的k会加大评估时间
    - 选择更大的k, 就会有更小的bias (因为训练集更加接近总数据集)
    - 在选择k时, 要最小化数据集之间的方差
    - 以上所有

24. 回归模型中存在多重共线性, 你如何解决这个问题？
    1) 去除这两个共线性变量
    2) 我们可以先去除一个共线性变量
    3) 计算VIF(方差膨胀因子), 采取相应措施
    4) 为了避免损失信息, 我们可以使用一些正则化方法, 比如, 岭回归和lasso回归
    
    - 1
    - 2
    - 2和3
    - 2, 3和4

25. 模型的高bias是什么意思, 我们如何降低它 ？
    - 在特征空间中减少特征
    - 在特征空间中增加特征
    - 增加数据点
    - 2和3
    - 以上所有

26. 对于信息增益, 决策树分裂节点, 下面说法正确的是（）
    1) 纯度高的节点需要更多的信息去区分
    2) 信息增益可以用”1比特-熵”获得
    3) 如果选择一个属性具有许多归类值, 那么这个信息增益是有偏差的
    
    - 1
    - 2
    - 2和3
    - 所有以上  

27. 假设我们要解决一个二类分类问题, 我们已经建立好了模型, 输出是0或1, 初始时设阈值为0.5, 超过0.5概率估计, 就判别为1, 
否则就判别为0 ; 如果我们现在用另一个大于0.5的阈值,  那么现在关于模型说法, 正确的是 : 
    1) 模型分类的召回率会降低或不变
    2) 模型分类的召回率会升高
    3) 模型分类准确率会升高或不变
    4) 模型分类准确率会降低
    
    - 1
    - 2
    - 1和3
    - 2和4
    - 以上都不是

28. “点击率问题”是这样一个预测问题, 99%的人是不会点击的, 而1%的人是会点击进去的, 所以这是一个非常不平衡的数据集. 
假设, 现在我们已经建了一个模型来分类, 而且有了99%的预测准确率, 我们可以下的结论是
    - 模型预测准确率已经很高了, 我们不需要做什么了
    - 模型预测准确率不高, 我们需要做点什么改进模型
    - 无法下结论
    - 以上都不对

29. 机器学习中，为何要经常对数据做归一化?
    ```
    一般做机器学习应用的时候大部分时间是花费在特征处理上，其中很关键的一步就是对特征数据进行归一化。
    
    原因：1）归一化后加快了梯度下降求最优解的速度；2）归一化有可能提高精度。
    
    简单扩展解释下这两点。
    1 归一化为什么能提高梯度下降法求解最优解的速度？
    因此如果机器学习模型使用梯度下降法求最优解时，归一化往往非常有必要，否则很难收敛甚至不能收敛。
    
    2 归一化有可能提高精度
    一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，
    从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）。
    
    3 归一化的类型
    1）线性归一化
    这种归一化方法比较适用在数值比较集中的情况。这种方法有个缺陷，如果max和min不稳定，很容易使得归一化结果不稳定，
    使得后续使用效果也不稳定。实际使用中可以用经验常量值来替代max和min。
    2）标准差标准化
    经过处理的数据符合标准正态分布，即均值为0，标准差为1
    3）非线性归一化
    经常用在数据分化比较大的场景，有些数值很大，有些很小。通过一些数学函数，将原始值进行映射。
    该方法包括 log、指数，正切等。需要根据数据分布的情况，决定非线性函数的曲线，比如log(V, 2)还是log(V, 10)等。
    ```

30. 请简要说说一个完整机器学习项目的流程
    ```
    1 抽象成数学问题
    明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
    这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，
    如果都不是的话，如果划归为其中的某类问题。
     
    2 获取数据
    数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
    数据要有代表性，否则必然会过拟合。
    而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
    而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，
    判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
     
    3 特征预处理与特征选择
    良好的数据要能够提取出良好的特征才能真正发挥效力。
    特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。
    归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。
    这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
    筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。
    特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，
    如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
     
    4 训练模型与调优
    直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。
    但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。
    理解越深入，就越能发现问题的症结，提出良好的调优方案。
     
    5 模型诊断
    如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
    过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。
    过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
    误差分析也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，
    是特征的问题还是数据本身的问题……
    诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
     
    6 模型融合
    一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
    工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。
    因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
     
    7 上线运行
    这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 
    不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
    这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。
    这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。
    ```

> 2  https://blog.csdn.net/abc_138/article/details/82855202

> 3  https://blog.csdn.net/abc_138/article/details/83025034

> 4  https://blog.csdn.net/abc_138/article/details/83272716

> 5  https://blog.csdn.net/abc_138/article/details/83479325

> 6  https://blog.csdn.net/abc_138/article/details/83721369

> 7  https://blog.csdn.net/abc_138/article/details/84500225

> others1  https://blog.csdn.net/ZHANG781068447/article/details/83713480

> others2  https://www.sohu.com/a/154630764_465975













    