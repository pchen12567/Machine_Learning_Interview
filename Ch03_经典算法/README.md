# 第三章 经典算法
- 不忘初心，方得始终。何谓“初心”？初心便是在深度学习、人工智能呼风唤雨的时代，对数据和结论之间那条朴素之路的永恒探寻，
是集前人之大智，真诚质朴求法向道的心中夙愿。
- 没有最好的分类器，只有最合适的分类器。
- 深度学习是数据驱动的，失去了数据，再精密的深度网络结构也是画饼充饥，无的放矢。
在很多实际问题中，很难得到海量且带有精确标注的数据，这时深度学习也就没有大显身手的余地，反而许多传统方法可以灵活巧妙地进行处理。
----
## 01 支持向量机

## 02 逻辑回归
> [逻辑回归实战](https://github.com/pchen12567/Machine_Learning_Interview/tree/master/Ch03_%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92)

逻辑回归（ Logistic Regression ）可以说是机器学习领域最基础也是最常用的模型，
逻辑回归的原理推导以及扩展应用几乎是算法工程师的必备技能。医生病理诊断、银行个人信用评估、邮箱分类垃圾邮件等，
无不体现逻辑回归精巧而广泛的应用。

### 逻辑回归中的相关概念
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_07.png?raw=true)
1. Sigmoid函数
    - 公式定义
    $$ S(x) = \frac{1}{1 + e^{-x}} $$
        - 当x = 0 时，S(x) = 0.5
        - 当x < 0 时，0 < S(x) < 0.5
        - 当x > 0 时，0.5 < S(x) < 1
    - 函数图像
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_06.jpg?raw=true)
    - 导数
    $$ S(x)'= S(x) \bigg( 1 - S(x) \bigg) $$
    - 导数推导过程
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_05.png?raw=true)

2. 熵<br>
在信息论中，熵（entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。
这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）
来自信源的另一个特征是样本的概率分布。
    - 信息熵<br>
    在信息论中，设离散随机变量$X$的分布为$ P(X=x^{(i)})=p_i , i=1,2,3,...,n $，则概率分布的熵的定义为：
    $$ H(p) = -\sum_{i=1}^n p_i \log{p_i} $$
    - 交叉熵<br>
    关于同一组事件$X_1,X_2,...,X_m$的两个分布$p$和$q$，(比如在逻辑回归中，p表示真实概率分布情况，q表示预测概率分布情况),
    其交叉熵的定义如下：
    $$ H(p,q) = - \sum_{i=1}^m  p_i \log(q_i) $$
    当两个分布完全相同时，交叉熵取最小值。<br>
    交叉熵可以衡量两个分布之间的相似度，交叉熵越小两个分布越相似。

3. 逻辑回归中的损失函数
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_08.png?raw=true)
 
### 逻辑回归相比于线性回归，有何异同？
- 首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。
- 逻辑回归中，因变量取值是一个二元分布，模型学习得出的是 $ E[y|x;\theta] $ ，
即给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预测分类问题。
而线性回归中实际上求解的是 $ y' = \theta^T x $，是对假设的真实关系 $ y = \theta^T x + \epsilon $的一个近似，
其中$ \epsilon $代表误差项，使用这个近似项来处理回归问题。
- 逻辑回归中的因变量为离散的，而线性回归中的因变量是连续的。并且在自变量$x$与超参数$theta$确定的情况下，
逻辑回归可以看作广义线性模型（Generalized Linear Models）在因变量$y$服从二元分布时的一个特殊情况；
而使用最小二乘法求解线性回归时，认为因变量$y$服从正态分布。
- 二者都使用了极大似然估计来对训练样本进行建模。线性回归使用最小二乘法，实际上就是在自变量$x$与超参数$theta$确定，
因变量$y$服从正态分布的假设下，使用极大似然估计的一个化筒。而逻辑回归中通过对似然函数
$ L(\theta) = \prod_{i=1}^m P(y_i | x_i; \theta)$ 的学习，得到最佳参数$\theta$。
- 另外，二者在求解超参数的过程中，都可以使用梯度下降的方法。

### 逻辑回归的损失函数及推导过程以及什么是交叉熵？
- **交叉熵** Cross Entropy <br>
关于同一组事件$X_1,X_2,...,X_m$的两个分布$p$和$q$，其交叉熵的定义如下：
$$ H(p,q) = - \sum_{i=1}^m  p_i \log(q_i) $$
当两个分布完全相同时，交叉熵取最小值。<br>
交叉熵可以衡量两个分布之间的相似度，交叉熵越小两个分布越相似。
- 损失函数推导过程<br>
目标是要最大化似然概率，即：
$$ \max \prod_{i=1}^m p(y_i|x_i; \theta) = \max \frac{1}{m} \prod_{i=1}^m 
[y^{(i)} \log{(h_\theta(x^{(i)}))} + (1 - y^{(i)}) \log{(1 - h_\theta(x^{(i)}))}] $$
取负对数，将$max$函数换成$min$函数，即：
$$ \min \quad -\frac{1}{m} \sum_{i=1}^m 
[y^{(i)} \log{(h_\theta(x^{(i)}))} + (1 - y^{(i)}) log{(1 - h_\theta{(x^{(i)})})}] $$
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_01.jpeg?raw=true)<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_04.png?raw=true)

### 逻辑回归应用于多分类问题时的情况？
多项逻辑回归实际上是二分类多级回归在多标签分类下的一种拓展。

二分类问题<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_02.png?raw=true)

多分类问题<br>
如果一个样本只对应于一个标签，可以假设每个样本属于不同标签的概率服从于几何分布，使用多项逻辑回归( Softmax Regression ) 来进行分类<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_03.png?raw=true)<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/lr_09.png?raw=true)<br>
其中$\theta_1, \theta_2,...,\theta_k \in R^n$为模型的参数，而$ \frac{1}{\sum_{j=1}^k e^{\theta_j^T x}}$可以看作是对概率的归一化。
为了方便起见，将$ {\theta_1, \theta_2,...,\theta_k} $这k个列向量按顺序排列成$n \times k$维矩阵，写作$\theta$，表示整个参数集。
一般来说，多项逻辑回归具有参数冗余的特点，即将$ \theta_1, \theta_2,...,\theta_k $同时加减一个向量后预测结果不变。

因此，多项逻辑回归实际上是二分类逻辑回归在多标签分类下的一种拓展。

当存在样本可能属于多个标签的情况时，可以训练k个二分类的逻辑回归分类器。第i个分类器用以区分每个样本是否可以归为第i类，
训练该分类器时，需要把标签重新整理为“第i类标签”与"非第i类标签”两类。遇过这样的办法，就解决了每个样本可能拥有多个标签的情况 。

## 03 决策树
> [决策树实战](https://github.com/pchen12567/Machine_Learning_Interview/tree/master/Ch03_%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/%E5%86%B3%E7%AD%96%E6%A0%91)

决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。
结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。
从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。
再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别（即叶结点）中。

决策树作为最基础、最常见的有监督学习模型，常被用于分类问题和回归问题，在市场营销和生物医药等领域尤其受欢迎，
主要因为树形结构与销售、诊断等场景下的决策过程十分相似。将决策树应用集成学习的思想可以得到随机森林、梯度提升决策树等模型。
完全生长的决策树模型具有简单直观、解释性强的特点，值得认真理解，这也是为融会贯通集成学习相关内容所做的铺垫。

一般而言，决策树的生成包含了特征选择、树的构造、树的剪枝三个过程。

### 决策树有哪些常用的启发函数？
从若干不同的决策树中选取最优的决策树是一个NP完全问题，在实际中通常会采用启发式学习的方法去构建一棵满足启发式条件的决策树。

常用的决策树算法有ID3、C4.5、CART。

- ID3--最大信息增益 <br>
对于样本集合D,类别K，数据集D的**经验熵**表示为：
$$ H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} $$
其中$C_k$是样本集合$D$中属于第$k$类的样本子集，$|C_k|$表示该子集的元素个数，$|D|$表示样本集合的元素个数。<br>
计算某个特征A对于数据集D的**经验条件熵**$H(D|A)$为：
$$ H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} = \sum_{i=1}^n \frac{|D_i|}{|D|} (-\sum_{k=1}^k \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}) $$
其中，$D_i$表示D中特征A取第i个值的样本子集，$D_{ik}$表示$D_i$中属于第k类的样本子集。<br>
于是**信息增益**$G(D,A)$可以表示为二者之差，得到：
$$ G(D|A) = H(D) - H(D|A) $$
在实际应用中，决策树往往不能通过一个特征就完成构建，需要在经验熵非0的类别中继续生长。

- C4.5--最大信息增益比 <br>
特征A对于数据集D的**信息增益比**$G_R(D,A)$定义为：
$$ G_R(D,A) = \frac{G(D,A)}{H_A(D)} $$
其中，$H_A(D)$称为数据集D关于A的**取值熵**
$$ H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|} $$

- CART(分类回归树)--最大基尼指数(Gini) <br>
Gini描述的是数据的纯度，与信息熵含义类似。
$$ Gini(D) = 1 - \sum_{k=1}^n (\frac{|C_k|}{|D|})^2 $$
CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。
但与ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树。<br>
特征A的Gini指数定义为:
$$ Gini(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i) $$

1. 首先，ID3是采用信息增益作为评价标准，除了“特征A”这一逆天特征外，会倾向于取值较多的特征。
因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大。
这在实际应用中是一个缺陷。比如，引入特征“DNA”，每个人的DNA都不同，如果ID3按照“DNA”特征进行划分一定是最优的（条件熵为0），
但这种分类的泛化能力是非常弱的。因此，C4.5实际上是对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，
避免ID3出现过拟合的特性，提升决策树的泛化能力。

2. 其次，从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。C4.5处理连续型变量时，
通过对数据排序之后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量。
而对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。

3. 从应用角度，ID3和C4.5只能用于分类任务，而CART（Classification and Regression Tree，分类回归树）
从名字就可以看出其不仅可以用于分类，也可以应用于回归任务（回归树使用最小平方误差准则）。

4. 此外，从实现细节、优化过程等角度，这三种决策树还有一些不同。
比如，ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；
ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，
因此最后会形成一颗二叉树，且每个特征可以被重复使用；ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，
而CART直接利用全部数据发现所有可能的树结构进行对比。

### 如何对决策树进行剪枝？
一棵完全生长的决策树会面临一个很严重的问题，即过拟合。因此需要对决策树进行剪枝，剪掉一些枝叶，提升模型的泛化能力。

决策树的剪枝通常有两种方法，预剪枝（Pre-Pruning）和后剪枝（Post-Pruning）。

- 预剪枝 <br>
预剪枝，即在生成决策树的过程中提前停止树的增长。<br>
预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树。
此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。预剪枝对于何时停止决策树的生长有以下几种方法。
    - 当树到达一定深度的时候，停止树的生长。
    - 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
    - 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。
    
    预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。
但如何准确地估计何时停止树的生长（即上述方法中的深度或阈值），针对不同问题会有很大差别，需要一定经验判断。
且预剪枝存在一定局限性，有欠拟合的风险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能会有显著上升。

- 后剪枝 <br>
后剪枝，是在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。<br>
后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。
剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。
同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。
相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。

    常见的后剪枝方法包括错误率降低剪枝（Reduced Error Pruning，REP）、悲观剪枝（Pessimistic Error Pruning，PEP）、
代价复杂度剪枝（Cost Complexity Pruning，CCP）、最小误差剪枝（Minimum Error Pruning，MEP）、
CVP（Critical Value Pruning）、OPP（Optimal Pruning）等方法，这些剪枝方法各有利弊，关注不同的优化角度。

    这里选取著名的CART剪枝方法CCP进行介绍。代价复杂剪枝主要包含以下两个步骤：
    - 从完整决策树$T_0$开始，生成一个子树序列${T_0,T_1,T_2,...,T_n}$，其中$T_{i+1}$由$T_i$生成，$T_n$为树的根结点。
    - 在子树序列中，根据真实误差选择最佳的决策树。
    
(To be Continue)

### 什么是奥卡姆剃刀定律？
通常表述为” 当两个假说具有完全相同的解释力和预测力时，我们以那个较为简单的假说作为讨论依据”。

奥卡姆剃刀的思想其实与机器学习消除过拟合的思想是一致的。特别是在决策树剪枝的过程中，我们正是希望在预测力不减的同时，
用一个简单的模型去替代原来复杂的模型。而在ID3决策树算法提出的过程中，模型的创建者也确实参照了奥卡姆剃刀的思想。
类似的思想还同样存在子神经网络的Dropout的方法中，我们降低模型复杂度，为的是提高模型的泛化能力。

严格讲，奥卡姆剃刀定律不是一个定理， 而是一种思考问题的方式。面对任何工作的时候，
如果有一个简单的方法和一个复杂的方法能够达到同样的效果，应该选择简单的那个。因为简单的选择是巧合的几率更小，
更有可能反应事物的内在规律。

## 04 K-近邻
### 什么是K-近邻算法？
KNN (k-NearestNeighbor)
- 是一种基于样本/实例的算法，K代表测试样本与训练样本最接近的个数
- 无参(nonparametric)模型
    - 参数通常分为超参数（无法通过机器学习出来，只能通过人为指定）和模型自身参数（可以通过机器学习出来的参数）。
    - 这里的无参模型的参数指的是模型自身的参数。
- 可用于分类和回归
- 步骤:
    - 计算出测试样本和所有训练样本的距离;
    - 为测试样本选择k个与其距离最小的训练样本; 
    - 统计出k个训练样本中大多数样本所属的分类; 
    - 这个分类就是待分类数据所属的分类
- Sk-Learn中K的默认值为5。
- k必须是奇数吗? 可以是偶数，除了通过数量判断，还可以通过距离判断，如将距离的倒数设置为权重。

### KNN中的距离是什么？
![](https://github.com/pchen12567/picture_store/blob/master/Interview/knn_01.png?raw=true)
1. 一般而言，定义一个距离函数 d(x,y), 需要满足下面几个准则：
    - d(x,x) = 0 // 到自己的距离为0 
    - d(x,y) >= 0 // 距离非负 
    - d(x,y) = d(y,x) // 对称性: 如果 A 到 B 距离是 a，那么 B 到 A 的距离也应该是 a 
    - d(x,k) d(k,y) >= d(x,y) // 三角形法则: (两边之和大于第三边)

2. 闵可夫斯基距离（Minkowski distance）
    - 闵氏距离不是一种距离，而是一组距离的定义。
    - 闵氏距离是衡量数值点之间距离的一种非常常见的方法。
    
    - 假设数值点$P$和$Q$坐标如下：
    $$ P=(x_1, x_2,..., x_n) \quad and \quad Q=(y_1, y_2,..., y_n) \in R^n $$
    那么，闵可夫斯基距离定义为:
    $$ d(x, y) = \bigg(\sum_{i=1}^n |x_i - y_i|^p \bigg)^{\frac{1}{p}} $$
    
    - 闵氏距离的缺点主要有两个：
        - 将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。
        - 没有考虑各个分量的分布（期望，方差等)可能是不同的。
        
    举个例子：二维样本(身高,体重)，其中身高范围是150-190，体重范围是50-60，有三个样本：a(180,50)，b(190,50)，c(180,60)。
    那么a与b之间的闵氏距离（无论是曼哈顿距离、欧氏距离或切比雪夫距离）等于a与c之间的闵氏距离，但是身高的10cm真的等价于体重的10kg么？
    因此用闵氏距离来衡量这些样本间的相似度很有问题。

3. 曼哈顿距离（ManhattanDistance）
    - 当P=1时，就是曼哈顿距离

4. 欧几里得距离（Euclidean distance）
    - 当P=2时，就是欧几里得距离
    - KNN默认距离是欧氏距离

5. 切比雪夫距离（Chebyshev distance）
    - 当P趋近于无穷大时，闵式距离转化成切比雪夫距离
    - $ \lim_{p \to +\infty} \bigg(\sum_{i=1}^n |x_i - y_i|^p \bigg)^{\frac{1}{p}} = \max_{i=1}^n |x_i - y_i| $

## 05 模型比较
### 支持向量机的特点
- 优点：泛化错误率低，计算开销不大，结果容易解释。
- 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改适用于处理二类问题。
- 使用数据类型：数值型和类别型数据

### 逻辑回归的特点
- 优点：计算代价不高，易于理解和实现。
- 缺点：容易欠拟合，分类精度可能不高。
- 使用数据类型：离散型数值型和类别型数据。
- 常用场景：医生病理诊断、银行个人信用评估、邮箱分类I主极邮件等。

### 决策树的特点
- 优点：计算复杂度不高，容易可视化易于理解，对中间值的缺失不敏感，无需对特征进行归一化处理。
- 缺点：可能产生过度匹配问题，即使剪枝后也很难避免过拟合，通常需要进行ensemble（如随机森林）才能达到比较好的效果。
- 适用数据类型：连续型数值型和类别型。
- 常用场景：常被用于分类问题和回归问题，在市场营销和生物医药等领域尤其受欢迎。

### K-近邻的特点
- 优点：算法简单直观，易于实现；不需要额外的数据，只依靠数据（样本）本身。
- 缺点：计算量较大，分类速度慢；需要预先指定K值。