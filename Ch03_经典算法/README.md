# 第三章 经典算法
- 不忘初心，方得始终。何谓“初心”？初心便是在深度学习、人工智能呼风唤雨的时代，对数据和结论之间那条朴素之路的永恒探寻，
是集前人之大智，真诚质朴求法向道的心中夙愿。
- 没有最好的分类器，只有最合适的分类器。
- 深度学习是数据驱动的，失去了数据，再精密的深度网络结构也是画饼充饥，无的放矢。
在很多实际问题中，很难得到海量且带有精确标注的数据，这时深度学习也就没有大显身手的余地，反而许多传统方法可以灵活巧妙地进行处理。
----
## 01 支持向量机

## 02 逻辑回归
### 逻辑回归相比于线性回归，有何异同？
- 首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。
- 逻辑回归中，因变量取值是一个二元分布，模型学习得出的是 $ E[y|x;\theta] $ ，
即给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预测分类问题。
而线性回归中实际上求解的是 $ y' = \theta^T x $，是对假设的真实关系 $ y = \theta^T x + \epsilon $的一个近似，
其中$ \epsilon $代表误差项，使用这个近似项来处理回归问题。

## 03 决策树
决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。
结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。
从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。
再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别（即叶结点）中。

决策树作为最基础、最常见的有监督学习模型，常被用于分类问题和回归问题，在市场营销和生物医药等领域尤其受欢迎，
主要因为树形结构与销售、诊断等场景下的决策过程十分相似。将决策树应用集成学习的思想可以得到随机森林、梯度提升决策树等模型。
完全生长的决策树模型具有简单直观、解释性强的特点，值得认真理解，这也是为融会贯通集成学习相关内容所做的铺垫。

一般而言，决策树的生成包含了特征选择、树的构造、树的剪枝三个过程。

### 决策树有哪些常用的启发函数？
从若干不同的决策树中选取最优的决策树是一个NP完全问题，在实际中通常会采用启发式学习的方法去构建一棵满足启发式条件的决策树。

常用的决策树算法有ID3、C4.5、CART。

- ID3--最大信息增益
对于样本集合D,类别K，数据集D的经验熵表示为：
$$ H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} $$
其中$C_k$是样本集合$D$中属于第$k$类的样本子集，$|C_k|$表示该子集的元素个数，$|D|$表示样本集合的元素个数。<br>
计算某个特征A对于数据集D的经验条件熵$H(D|A)$为：
$$ H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} = \sum_{i=1}^n \frac{|D_i|}{|D|} (-\sum_{k=1}^k \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}) $$
其中，$D_i$表示D中特征A取第i个值的样本子集，$D_ik$表示$D_i$中属于第k类的样本子集。<br>
于是信息增益$g(D,A)$可以表示为二者之差，得到：
$$ g(D|A) = H(D) - H(D|A) $$
在实际应用中，决策树往往不能通过一个特征就完成构建，需要在经验熵非0的类别中继续生长。

- C4.5--最大信息增益比
特征A对于数据集D的信息增益比定义为：
$$ g_R(D,A) = \frac{g(D,A)}{H_A(D)} $$
其中，$H_A(D)$称为数据集D关于A的取值熵
$$ H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|} $$

- CART--最大基尼指数(Gini)
Gini描述的是数据的纯度，与信息熵含义类似。
$$ Gini(D) = 1 - \sum_{k=1}^n (\frac{|C_k|}{|D|})^2 $$
CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。
但与ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树。<br>
特征A的Gini指数定义为:
$$ Gini(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i) $$

1. 首先，D3是采用信息增益作为评价标准，除了“特征A”这一逆天特征外，会倾向于取值较多的特征。
因为，信息增益反映的是给定条件以后不确定性减少的程度，特征取值越多就意味着确定性更高，也就是条件熵越小，信息增益越大。
这在实际应用中是一个缺陷。比如，引入特征“DNA”，每个人的DNA都不同，如果ID3按照“DNA”特征进行划分一定是最优的（条件熵为0），
但这种分类的泛化能力是非常弱的。因此，C4.5实际上是对ID3进行优化，通过引入信息增益比，一定程度上对取值比较多的特征进行惩罚，
避免ID3出现过拟合的特性，提升决策树的泛化能力。

2. 其次，从样本类型的角度，ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量。C4.5处理连续型变量时，
通过对数据排序之后找到类别不同的分割线作为切分点，根据切分点把连续属性转换为布尔型，从而将连续型变量转换多个取值区间的离散型变量。
而对于CART，由于其构建时每次都会对特征进行二值划分，因此可以很好地适用于连续性变量。

3. 从应用角度，ID3和C4.5只能用于分类任务，而CART（Classification and Regression Tree，分类回归树）
从名字就可以看出其不仅可以用于分类，也可以应用于回归任务（回归树使用最小平方误差准则）。

4. 此外，从实现细节、优化过程等角度，这三种决策树还有一些不同。
比如，ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理；
ID3和C4.5可以在每个结点上产生出多叉分支，且每个特征在层级之间不会复用，而CART每个结点只会产生两个分支，
因此最后会形成一颗二叉树，且每个特征可以被重复使用；ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，
而CART直接利用全部数据发现所有可能的树结构进行对比。

### 如何对决策树进行剪枝？
一棵完全生长的决策树会面临一个很严重的问题，即过拟合。因此需要对决策树进行剪枝，剪掉一些枝叶，提升模型的泛化能力。

决策树的剪枝通常有两种方法，预剪枝（Pre-Pruning）和后剪枝（Post-Pruning）。

- 预剪枝
预剪枝，即在生成决策树的过程中提前停止树的增长。<br>
预剪枝的核心思想是在树中结点进行扩展之前，先计算当前的划分是否能带来模型泛化能力的提升，如果不能，则不再继续生长子树。
此时可能存在不同类别的样本同时存于结点中，按照多数投票的原则判断该结点所属类别。预剪枝对于何时停止决策树的生长有以下几种方法。
    - 当树到达一定深度的时候，停止树的生长。
    - 当到达当前结点的样本数量小于某个阈值的时候，停止树的生长。
    - 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，不再继续扩展。
    
    预剪枝具有思想直接、算法简单、效率高等特点，适合解决大规模问题。
但如何准确地估计何时停止树的生长（即上述方法中的深度或阈值），针对不同问题会有很大差别，需要一定经验判断。
且预剪枝存在一定局限性，有欠拟合的风险，虽然当前的划分会导致测试集准确率降低，但在之后的划分中，准确率可能会有显著上升。

- 后剪枝
后剪枝，是在已生成的过拟合决策树上进行剪枝，得到简化版的剪枝决策树。<br>
后剪枝的核心思想是让算法生成一棵完全生长的决策树，然后从最底层向上计算是否剪枝。
剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样按照多数投票的原则进行判断。
同样地，后剪枝也可以通过在测试集上的准确率进行判断，如果剪枝过后准确率有所提升，则进行剪枝。
相比于预剪枝，后剪枝方法通常可以得到泛化能力更强的决策树，但时间开销会更大。

常见的后剪枝方法包括错误率降低剪枝（Reduced Error Pruning，REP）、悲观剪枝（Pessimistic Error Pruning，PEP）、
代价复杂度剪枝（Cost Complexity Pruning，CCP）、最小误差剪枝（Minimum Error Pruning，MEP）、
CVP（Critical Value Pruning）、OPP（Optimal Pruning）等方法，这些剪枝方法各有利弊，关注不同的优化角度。

这里选取著名的CART剪枝方法CCP进行介绍。代价复杂剪枝主要包含以下两个步骤：
    - 从完整决策树$T_0$开始，生成一个子树序列${T_0,T_1,T_2,...,T_n}$，其中$T_{i+1}$由$T_i$生成，$T_n$为树的根结点。
    - 在子树序列中，根据真实误差选择最佳的决策树。
    
(To be Continue)