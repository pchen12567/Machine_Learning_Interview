# 第三章 经典算法
- 不忘初心，方得始终。何谓“初心”？初心便是在深度学习、人工智能呼风唤雨的时代，对数据和结论之间那条朴素之路的永恒探寻，
是集前人之大智，真诚质朴求法向道的心中夙愿。
- 没有最好的分类器，只有最合适的分类器。
- 深度学习是数据驱动的，失去了数据，再精密的深度网络结构也是画饼充饥，无的放矢。
在很多实际问题中，很难得到海量且带有精确标注的数据，这时深度学习也就没有大显身手的余地，反而许多传统方法可以灵活巧妙地进行处理。
----
## 01 支持向量机

## 02 逻辑回归
### 逻辑回归相比于线性回归，有何异同？
- 首先，逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者的最本质的区别。
- 逻辑回归中，因变量取值是一个二元分布，模型学习得出的是 $ E[y|x;\theta] $ ，
即给定自变量和超参数后，得到因变量的期望，并基于此期望来处理预测分类问题。
而线性回归中实际上求解的是 $ y' = \theta^T x $，是对假设的真实关系 $ y = \theta^T x + \epsilon $的一个近似，
其中$ \epsilon $代表误差项，使用这个近似项来处理回归问题。

## 03 决策树
决策树是一种自上而下，对样本数据进行树形分类的过程，由结点和有向边组成。
结点分为内部结点和叶结点，其中每个内部结点表示一个特征或属性，叶结点表示类别。
从顶部根结点开始，所有样本聚在一起。经过根结点的划分，样本被分到不同的子结点中。
再根据子结点的特征进一步划分，直至所有样本都被归到某一个类别（即叶结点）中。

决策树作为最基础、最常见的有监督学习模型，常被用于分类问题和回归问题，在市场营销和生物医药等领域尤其受欢迎，
主要因为树形结构与销售、诊断等场景下的决策过程十分相似。将决策树应用集成学习的思想可以得到随机森林、梯度提升决策树等模型。
完全生长的决策树模型具有简单直观、解释性强的特点，值得认真理解，这也是为融会贯通集成学习相关内容所做的铺垫。

一般而言，决策树的生成包含了特征选择、树的构造、树的剪枝三个过程。

### 决策树有哪些常用的启发函数？
从若干不同的决策树中选取最优的决策树是一个NP完全问题，在实际中通常会采用启发式学习的方法去构建一棵满足启发式条件的决策树。

常用的决策树算法有ID3、C4.5、CART。

- ID3--最大信息增益
对于样本集合D,类别K，数据集D的经验熵表示为：
$$ H(D) = - \sum_{k=1}^K \frac{|C_k|}{|D|} \log_2 \frac{|C_k|}{|D|} $$
其中$C_k$是样本集合$D$中属于第$k$类的样本子集，$|C_k|$表示该子集的元素个数，$|D|$表示样本集合的元素个数。<br>
计算某个特征A对于数据集D的经验条件熵$H(D|A)$为：
$$ H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} = \sum_{i=1}^n \frac{|D_i|}{|D|} (-\sum_{k=1}^k \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}) $$
其中，$D_i$表示D中特征A取第i个值的样本子集，$D_ik$表示$D_i$中属于第k类的样本子集。<br>
于是信息增益$g(D,A)$可以表示为二者之差，得到：
$$ g(D|A) = H(D) - H(D|A) $$
在实际应用中，决策树往往不能通过一个特征就完成构建，需要在经验熵非0的类别中继续生长。

- C4.5--最大信息增益比
特征A对于数据集D的信息增益比定义为：
$$ g_R(D,A) = \frac{g(D,A)}{H_A(D)} $$
其中，$H_A(D)$称为数据集D关于A的取值熵
$$ H_A(D) = -\sum_{i=1}^n \frac{|D_i|}{|D|} \log_2 \frac{|D_i|}{|D|} $$

- CART--最大基尼指数(Gini)
Gini描述的是数据的纯度，与信息熵含义类似。
$$ Gini(D) = 1 - \sum_{k=1}^n (\frac{|C_k|}{|D|})^2 $$
CART在每一次迭代中选择基尼指数最小的特征及其对应的切分点进行分类。
但与ID3、C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树。<br>
特征A的Gini指数定义为:
$$ Gini(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i) $$