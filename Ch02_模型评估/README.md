# 第二章 模型评估
- 只有选择与问题相匹配的评估方法，才能快速地发现模型选择或训练过程中出现的问题，迭代地对模型进行优化。
- 模型评估主要分为离线评估和在线评估两个阶段。
- 针对分类、排序、回归、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。
- 知道每种评估指标的精确定义、有针对性地选择合适的评估指标、根据评估指标的反馈进行模型调整，
这些都是机器学习在模型评估阶段的关键问题，也是一名合格的算法工程师应当具备的基本功。
----
## 01 评估指标的局限性
在模型评估过程中，分类问题、排序问题、回归问题往往需要使用不同的指标进行评估。
在诸多的评估指标中，大部分指标只能片面地反映模型的一部分性能。
如果不能合理地运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。

准确率（Accuracy）<br>
精确率（Precision）<br>
召回率（Recall）<br>
均方根误差（MSE）
![](https://github.com/pchen12567/picture_store/blob/master/AI_For_NLP/validation.png?raw=true)

### 准确率的局限性
- 准确率：指分类正确的样本占总样本个数的比例
$$ Accuracy = \frac{n_{correct}}{n_{total}} $$
- 准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。
比如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。
所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准确率的最主要因素。
- 为了解决这个问题，可以使用更为有效的平均准确率（每个类别下的样本准确率的算术平均）作为模型评估的指标。
- 标准答案其实也不限于指标的选择，即使评估指标选择对了，
仍会存在模型过拟合或欠拟合、测试集和训练集划分不合理、线下评估与线上测试的样本分布存在差异等一系列问题，
但评估指标的选择是最容易被发现，也是最可能影响评估结果的因素。

### 精确率与召回率的权衡
- 精确率：分类正确的正样本个数占分类器判定为正样本的样本个数的比例。
- 召回率：分类正确的正样本个数占真正的正样本个数的比例。
- Precision值和Recall值是既矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在“更有把握”时才把样本预测为正样本，
但此时往往会因为过于保守而漏掉很多“没有把握”的正样本，导致Recall值降低。
- 通常使用F1值，即精确率和召回率的调和平均值，来综合反映一个模型的性能。

## 02 ROC曲线
二值分类器（Binary Classifier）是机器学习领域中最常见也是应用最广泛的分类器。
评价二值分类器的指标很多，比如precision、recall、F1 score、P-R曲线等。
但也发现这些指标或多或少只能反映模型在某一方面的性能。
相比而言，ROC曲线则有很多优点，经常作为评估二值分类器最重要的指标之一。

### 什么是ROC曲线？
- ROC曲线是Receiver Operating Characteristic Curve的简称，中文名为“受试者工作特征曲线”。
- ROC曲线的横坐标为假阳性率（False Positive Rate，FPR）；纵坐标为真阳性率（True Positive Rate，TPR）。
$$ FPR = \frac{FP}{N} $$
$$ TPR = \frac{TP}{P} $$
P是真实的正样本的数量，N是真实的负样本的数量，TP是P个正样本中被分类器预测为正样本的个数，FP是N个负样本中被分类器预测为正样本的个数。