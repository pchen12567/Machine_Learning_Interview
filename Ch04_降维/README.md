# 第四章 降维
- 机器学习中的数据维数与现实世界的空间维度本同末离。
- 在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对向维向量进行处理和分析时，会极大地消耗系统资源，
甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。
- 常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影等。
----
## 01 PCA最大方差理论
在机器学习领域中，对原始数据进行特征提取，有时会得到比较高维的特征向量。
在这些向量所处的高维空间中，包含很多的冗余和噪声。我们希望通过降维的方式来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。
主成分分析（Principal Components Analysis，PCA）作为降维中最经典的方法，至今已有100多年的历史，
它属于一种线性、非监督、全局的降维算法，是面试中经常被问到的问题。

> [参考：PCA的数学原理(重点学习)](https://zhuanlan.zhihu.com/p/21580949)
>
> [参考：如何理解PCA主成分分析](https://www.zhihu.com/question/41120789/answer/481966094)

### PCA的原理
1. 特征降维
    - 用于减少数据集的维度，同时保持数据集中的对方差贡献最大的特征。
    - 保留低阶主成分，忽略高阶成分，这样的低阶成分往往能够保留住数据的最重要方面。
    - 通过线型变换将原数据映射到新的坐标系统中，使映射后的第一个坐标上的方差最大（即第一个主成分）,
    第二个坐标上的方差第二大（即第二个主成分）...<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_01.png?raw=true)

2. 特征选择与特征降维的区别
    - 特征选择是从全部的特征中选择出一部分特征，是全部特征的一个子集。
    - 特征降维是采用坐标转换的方式，将全部特征的维度从一个高维度转换成低维度。

3. 方差与协方差
    - 用于衡量一系列点在它们的重心或均值附近的分散程度。
    - 方差：衡量这些点在一个维度的偏差。
    - **协方差：衡量一个维度是否会对另一个维度有所影响，从而查看这两个维度之间是否有关系。**
    - 某个维度和自身之间的协方差就是其方差。
    $$ cov(X,Y) =  \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{n - 1} $$

4. 协方差矩阵
    - 如果数据集是d维的，(x1, x2, …, xd)，则可计算出(x1, x2), (x1, x3), …, (x1, xd), (x2, x3), …(x2, xd), …(xd-1, xd)之间的协方差。
    由于协方差的对称性，再加上各维度自身的协方差，可以构成协方差矩阵。
    - 其中对角线上的是方差。
    - 协方差为正,代表两个变量变化趋势相同；反之亦然。<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_02.png?raw=true)

5. PAC步骤
    1. 数据集$ X \in R^{m \times n} $，其中每个样本 $ X^{(i)} = [X_1^{(i)}, X_2^{(i)},...,X_n^{(i)}], \quad i \in m$。
    计算每个维度的均值：
    $$ \bar{X} = \frac{1}{m} \sum_{i=1}^m [ X_1^{(i)}, X_2^{(i)},...,X_n^{(i)} ] \in R^n $$
    每个维度减去这个均值，得到一个矩阵，相当于将坐标系进行了平移：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_03.png?raw=true)
    
    2. 构建协方差矩阵：<br>
    ![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_04.png?raw=true)
    
    3. 矩阵分解（如SVD），得到特征值(eigenvalues)为标量$\lambda$以及特征向量(eigenvectors)为向量$V$。
    
    4. 将特征值从大到小排序，对应的特征向量就是第一个主成分，第二个主成分...

6. 如何选择主成分个数
    1. 交叉验证
    2. 根据主成分的累计贡献率(t)：
    $$ \frac{\sum_{i=1}^d \lambda_i}{\sum_{i=1}^n \lambda_i} \geq t $$

7. 应用：特征提取、数据降维
    
### 如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对PCA问题进行求解？
PCA旨在找到数据中的主成分，并利用这些主成分表征原始数据，从而达到降维的目的。举一个简单的例子，在三维空间中有一系列数据点，
这些点分布在一个过原点的平面上。如果我们用自然坐标系x,y,z三个轴来表示数据，就需要使用三个维度。而实际上，
这些点只出现在一个二维平面上，如果我们通过坐标系旋转变换使得数据所在平面与x,y平面重合，那么我们就可以通过x′,y′两个维度表达原始数据，
并且没有任何损失，这样就完成了数据的降维。而x′,y′两个轴所包含的信息就是我们要找到的主成分。

在信号处理领域，认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比。信噪比越大意味着数据的质量越好，
反之，信噪比越小意味着数据的质量越差。由此不难引出PCA的目标，即最大化投影方差，也就是让数据在主轴上投影的方差最大。

对于给定的一组数据点$ {v_1,v_2,...,v_n} $，其中所有向量均为列向量，中心化后的表示为
$ {x_1,x_2,...,x_n} = {v_1 - \mu, v_2 - \mu,...,v_n - \mu} $，其中$ \mu = \frac{1}{n} \sum_{i=1}{n} v_i $。
我们知道，向量内积在几何上表示为第一个向量投影到第二个向量上的长度，因此向量$x_i$在$w$(单位方向向量)上的投影坐标可以表示为
$ (x_i, w) = x_i^T w $。所以，目标是找到一个投影方向$w$，使得$ x_1, x_2,...,x_n$在$w$上的投影方差尽可能大。
易知，投影之后均值为0，前面中心化的意义所在，因此投影后的方差可以表示为：
$$ D(x) = \frac{1}{n} \sum_{i=1}^n (x_i^T w - 0)^2 = \frac{1}{n} \sum_{i=1}^T (x_i^T w)^T (x_i^T w) $$
$$ => D(x) = \frac{1}{n} \sum_{i=1}^n w^T x_i x_i^T w = w^T \bigg( \frac{1}{n} \sum_{i=1}^n x_i x_i^T \bigg) w $$
仔细观察可以发现，$ \bigg( \frac{1}{n} \sum_{i=1}^n x_i x_i^T \bigg) $其实就是样本协方差矩阵，将其写作$ \Sigma $，即：
$$ \Sigma = \bigg( \frac{1}{n} \sum_{i=1}^n x_i x_i^T \bigg) $$
另外，由于$w$是单位方向向量，即有$ w^T w = 1 $。因此要求解一个最大化问题，可以表示为：
$$ \max \lbrace w^T \Sigma w \rbrace, \quad s.t.\ w^T w = 1 $$
引入拉格朗日乘子，并对$w$求导令其等于0，得到：
$$ F(w,\lambda) = (w^T \Sigma w) + \lambda(w^T w - 1)  $$
$$ => \frac{\partial F(w, \lambda)}{\partial w} = 2\Sigma w + 2\lambda w = 0 $$
$$ => \Sigma w = - \lambda w $$
这里令$-\lambda$为特征值，便可以推出$ \Sigma w = \lambda w $，此时：
$$ D(x) =w^T \Sigma w = \lambda w^T w = \lambda $$
观察可以发现，$x$投影之后的方差就是协方差矩阵的特征值，要找到最大的方差也就是协方差矩阵最大的特征值，
最佳投影方向就是最大特征值所对应的特征向量。次佳投影方向位于最佳投影方向的正交空间中，是第二大特征值对应的特征向量，以此类推。

至此，得到以下几种PCA的求解方法：
1. 对样本数据进行中心化处理。
2. 求样本协方差矩阵。
3. 对协方差矩阵进行特征值分解，将特征值从大到小排列。
4. 取特征值前$d$大对应的特征向量$w_1,w_2,...,w_d$，通过以下映射将$n$维样本映射到$d$维。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_05.png?raw=true)<br>
新的$x'_i$的第$d$维就是$x_i$在第$d$个主成分$w_d$方向上的投影，通过选取最大的$d$个特征值对应的特征向量，
将方差较小的特征(噪声)抛弃，使得每个$n$维列向量$x_i$被映射为$d$维列向量$x'_i$，定义降维后的信息占比为：
$$ \eta = \sqrt{ \frac{\sum\_{i=1}^d \lambda_i^2} {\sum\_{i=1}^n \lambda_i^2} } $$

至此，从最大化投影方差的角度解释了PCA的原理、目标函数和求解方法。其实，PCA还可以用其他思路进行分析，
比如从最小回归误差的角度得到新的目标函数。但最终会发现其对应的原理和求解方法与本文中的是等价的。
另外，由于PCA是一种线性降维方法，虽然经典，但具有一定的局限性。可以通过核映射对PCA进行扩展得到核主成分分析（KPCA），
也可以通过流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些PCA效果不好的复杂数据集进行非线性降维操作。

## 02 PCA最小平方误差理论
### PCA求解的其实是最佳投影方向，即一条直线，这与数学中线性回归问题的目标不谋而合，能否从回归的角度定义PCA的目标并相应地求解问题？
还是考虑二维空间中的样本点，上一节求解得到一条直线使得样本点投影到该直线上的方差最大。从求解直线的思路出发，
很容易联想到数学中的线性回归问题，其目标也是求解一个线性函数使得对应直线能够更好地拟合样本点集合。
如果从这个角度定义PCA的目标，那么问题就会转化为一个回归问题。

顺着这个思路，在高维空间中，实际上是要找到一个$d$维超平面，使得数据点到这个超平面的距离平方和最小。
以$d=1$为例，超平面退化为直线，即把样本点投影到最佳直线，最小化的就是所有点到直线的距离平方之和。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_06.png?raw=true)

数据集中每个点$x_k$到$d$维超平面$D$的距离为：
$$ distance(x_k, D) = || x_k - \tilde{x_k} ||\_2$$
其中$\tilde{x_k}$表示$x_k$在超平面$D$上的投影向量。如果该超平面由$d$个标准正交基$ W= \lbrace w_1, w_2, ..., w_d \rbrace $构成，
根据线性代数理论$\tilde{x_k}$可以由这组基线性表示为：
$$ \tilde{x_k} = \sum_{i=1}^d (w_i^T x_k) w_i $$
其中$w_i^T x_k$表示$x_k$在$w_i$方向上投影的长度。因此，$\tilde{x_k}$实际上就是$x_k$在$W$这组标准正交基下的坐标。
而PCA要优化的目标为：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/PCA_07.png?raw=true)

To be continue...

至此，从最小平方误差的角度解释了PCA的原理、目标函数和求解方法。不难发现，这与最大方差角度殊途同归，
从不同的目标函数出发，得到了相同的求解方法。

## 03 线性判别分析 LDA
线性判别分析（Linear Discriminant Analysis，LDA）是一种有监督学习算法，同时经常被用来对数据进行降维。
它是Ronald Fisher在1936年发明的，有些资料上也称之为Fisher LDA（Fisher’s Linear Discriminant Analysis）。
LDA是目前机器学习、数据挖掘领域中经典且热门的一种算法。

相比于PCA，LDA可以作为一种有监督的降维算法。在PCA中，算法没有考虑数据的标签（类别），只是把原数据映射到一些方差比较大的方向上而已。

假设用不同的颜色标注C1、C2两个不同类别的数据，如图所示。根据PCA算法，数据应该映射到方差最大的那个方向，亦即y轴方向。
但是，C1，C2两个不同类别的数据就会完全混合在一起，很难区分开。所以，使用PCA算法进行降维后再进行分类的效果会非常差。
但是，如果使用LDA算法，数据会映射到x轴方向。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/LDA_01.png?raw=true)

### 对于具有类别标签的数据，应当如何设计目标函数使得降维的过程中不损失列表信息？在这种目标下，应当如何进行求解？
LDA首先是为了分类服务的，因此只要找到一个投影方向$w$，使得投影后的样本尽可能按照原始分类分开。

先从一个简单的二分类问题出发，有$C_1$、$C_2$ 两个类别的样本，两类的均值分布为：
$$ \mu_1 = \frac{1}{N_1} \sum_{x \in C_1} x, \quad \mu_2 = \frac{1}{N_2} \sum_{x \in C_2} x $$
希望投影之后两类之间的距离尽可能大，距离表示为：
$$ D(C_1, C_2) = || \tilde{\mu_1} - \tilde{\mu_2} ||_2^2 $$
其中$\tilde{\mu_1}$ ，$\tilde{\mu_2}$表示两类的中心在$w$方向上的投影向量，$\tilde{\mu_1} = w^T \mu_1, \tilde{\mu_2} = w^T \mu_2$，
因此需要优化的问题为：<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/LDA_02.png?raw=true)<br>
容易发现，当$w$方向与$(\mu_1 - \mu_2)$一致的时候，该距离达到最大值。
如对图(a)中的黄棕两种类别的样本点进行降维时，若按照最大化两类投影中心距离的准则，会将样本点投影到下方的黑线上。
但是原本可以被线性划分的两类样本，经过投影后有了一定程度的重叠，这显然不能使我们满意。
我们希望得到的投影结果如图(b)所示，虽然两类的中心在投影之后的距离有所减小，但确使投影之后样本的可区分性提高了。<br>
![](https://github.com/pchen12567/picture_store/blob/master/Interview/LDA_03.png?raw=true)

仔细观察两种投影方式的区别，可以发现，在图(b)中，投影后的样本点似乎在每一类中分布得更为集中了，
用数学化的语言描述就是每类内部的方差比左图中更小。这就引出了LDA的中心思想——**最大化类间距离和最小化类内距离**。

在前文中已经找到了使得类间距离尽可能大的投影方式，现在只需要同时优化类内方差，使其尽可能小。
将整个数据集的类内方差定义为各个类分别的方差之和，将目标函数定义为类间距离和类内距离的比值，于是引出需要最大化的目标：
$$ \max_w J(w) = \frac{||w^T (\mu_1 - \mu_2) ||\_2^2}{D_1 + D_2} $$
其中$w$为单位向量，$D_1$，$D_2$分别表示两类投影后的方差
$$ D_1 = \sum_{x \in C_1}(w^T x - w^T \mu)^2 = \sum_{x \in C_1} w^T (x - \mu_1)(x - \mu_1)^T w $$
$$ D_2 = \sum_{x \in C_2}(w^T x - w^T \mu)^2 = \sum_{x \in C_2} w^T (x - \mu_2)(x - \mu_2)^T w $$
因此$J(w)$可以写成：
$$ J(w) = \frac{w^T (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T w}{\sum_{x \in C_i} w^T (x - \mu_i)(x - \mu_i)^T w} $$
定义类间散度矩阵为：
$$ S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T $$
定义类内散度矩阵为：
$$ S_w = \sum_{x \in C_i} (x - \mu_i)(x - \mu_i)^T $$
则$J(w)$可以写成：
$$ J(w) = \frac{w^T S_B w}{w^T S_w w} $$
要最大化$J(w)$ ,只需要对$w$求偏导，并令导数等于零：
$$ \frac{\partial J(w)}{\partial w} = \frac{\bigg[ \frac{\partial (w^T S_B w)}{\partial w} (w^T S_w w) - \frac{\partial (w^T S_w w)}{\partial w} (w^T S_B w) \bigg]}{(w^T S_w w)^2} = 0 $$
于是得到：
$$ => \frac{S_B w (w^T S_w w) - S_w w (w^T S_B w)}{(w^T S_B w)^2} = 0 $$
$$ => (w^T S_w w) S_B w = (w^T S_B w) S_w w $$
由于在简化的二分类问题中$ w^T S_w w $和$ w^T S_B w $是两个数，令：
$$ \lambda = J(w) = \frac{w^T S_B w}{w^T S_w w} $$
于是可以把上式写成如下形式：
$$ S_B w = \lambda S_w w $$
整理得到:
$$ S_w^{-1} S_B w = \lambda w $$

从这里可以看出，最大化的目标对应了一个矩阵的特征值，于是LDA降维变成了一个求矩阵特征向量的问题。
$J(ω)$就对应了矩阵$S_w^{-1} S_B$最大的特征值，而投影方向就是这个特征值对应的特征向量。

对于二分类这一问题，由于$S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$，可以得到：
$$ S_B w = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T w $$
由于$ (\mu_1 - \mu_2)^T w $的结果是一个常数值，这里令其值为C，得到：
$$ S_B w = C (\mu_1 - \mu_2) $$
因此$S_B w$的方向始终与$(\mu_1 - \mu_2)$一致。

由 $ S_w^{-1} S_B w = \lambda w $ 可以得到：
$$ S_w^{-1} C (\mu_1 - \mu_2) = \lambda w $$
$$ => w = \frac{C}{\lambda} S_w^{-1} (\mu_1 - \mu_2) $$
如果只考虑$w$的方向，不考虑其长度，可以将常数项$ \frac{C}{\lambda} $去掉，得到：
$$ w = S_w^{-1}(\mu_1 - \mu_2) $$
换句话说，只需要求样本的均值和类内方差，就可以马上得出最佳的投影方向。这便是Fisher在1936年提出的线性判别分析。

至此，从最大化类间距离、最小化类内距离的思想出发，推导出了LDA的优化目标以及求解方法。
Fisher LDA相比PCA更善于对有类别信息的数据进行降维处理，但它对数据的分布做了一些很强的假设，
例如，每个类数据都是高斯分布、各个类的协方差相等。尽管这些假设在实际中并不一定完全满足，但LDA已被证明是非常有效的一种降维方法。
主要是因为线性模型对于噪声的鲁棒性比较好，但由于模型简单，表达能力有一定局限性，
可以通过引入核函数扩展LDA方法以处理分布较为复杂的数据。

## 04 线性判别分析与主成分分析



