# 第四章 降维
机器学习中的数据维数与现实世界的空间维度本同末离。
在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对向维向量进行处理和分析时，会极大地消耗系统资源，
甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。
常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影等。
----
## 01 PCA最大方差理论
在机器学习领域中，对原始数据进行特征提取，有时会得到比较高维的特征向量。
在这些向量所处的高维空间中，包含很多的冗余和噪声。我们希望通过降维的方式来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。
主成分分析（Principal Components Analysis，PCA）作为降维中最经典的方法，至今已有100多年的历史，
它属于一种线性、非监督、全局的降维算法，是面试中经常被问到的问题。

### 如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对PCA问题进行求解？
