# 第四章 降维
机器学习中的数据维数与现实世界的空间维度本同末离。
在机器学习中，数据通常需要被表示成向量形式以输入模型进行训练。但众所周知，对向维向量进行处理和分析时，会极大地消耗系统资源，
甚至产生维度灾难。因此，进行降维，即用一个低维度的向量表示原始高维度的特征就显得尤为重要。
常见的降维方法有主成分分析、线性判别分析、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影等。
----
## 01 PCA最大方差理论
在机器学习领域中，对原始数据进行特征提取，有时会得到比较高维的特征向量。
在这些向量所处的高维空间中，包含很多的冗余和噪声。我们希望通过降维的方式来寻找数据内部的特性，从而提升特征表达能力，降低训练复杂度。
主成分分析（Principal Components Analysis，PCA）作为降维中最经典的方法，至今已有100多年的历史，
它属于一种线性、非监督、全局的降维算法，是面试中经常被问到的问题。

### PCA的原理
1. 特征降维
    - 用于减少数据集的维度，同时保持数据集中的对方差贡献最大的特征。
    - 保留低阶主成分，忽略高阶成分，这样的低阶成分往往能够保留住数据的最重要方面。
    - 通过线型变换将原数据映射到新的坐标系统中，使映射后的第一个坐标上的方差最大（即第一个主成分）,
    第二个坐标上的方差第二大（即第二个主成分）...<br>
    ![]()

2. 特征选择与特征降维的区别
    - 特征选择是从全部的特征中选择出一部分特征，是全部特征的一个子集。
    - 特征降维是采用坐标转换的方式，将全部特征的维度从一个高维度转换成低维度。

3. 方差与协方差
    - 用于衡量一系列点在它们的重心或均值附近的分散程度。
    - 方差：衡量这些点在一个维度的偏差。
    - **协方差：衡量一个维度是否会对另一个维度有所影响，从而查看这两个维度之间是否有关系。**
    - 某个维度和自身之间的协方差就是其方差。
    $$ cov(X,Y) =  \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{n - 1} $$

4. 协方差矩阵
    - 如果数据集是d维的，(x1, x2, …, xd)，则可计算出(x1, x2), (x1, x3), …, (x1, xd), (x2, x3), …(x2, xd), …(xd-1, xd)之间的协方差。
    由于协方差的对称性，再加上各维度自身的协方差，可以构成协方差矩阵。
    - 其中对角线上的是方差。
    - 协方差为正,代表两个变量变化趋势相同；反之亦然。<br>
    ![]()

5. PAC步骤
    1. 数据集$ X \in R^{m \times n} $，其中每个样本 $ X^{(i)} = [X_1^{(i)}, X_2^{(i)},...,X_n^{(i)}], \quad i \in m$。
    计算每个维度的均值：
    $$ \bar{X} = \frac{1}{m} \sum_{i=1}^m [ X_1^{(i)}, X_2^{(i)},...,X_n^{(i)} ] \in R^n $$
    每个维度减去这个均值，得到一个矩阵，相当于将坐标系进行了平移：
    $$ \begin{bmatrix} X^{(1)} - \bar{X} \\ X^{(2)} - \bar{X} \\ ... \\ X^{(m)} - \bar{X} \end{bmatrix} $$
    
### 如何定义主成分？从这种定义出发，如何设计目标函数使得降维达到提取主成分的目的？针对这个目标函数，如何对PCA问题进行求解？
